# --------------------------------------------------------------------------------------------------
# Basic metadata
# --------------------------------------------------------------------------------------------------
code: h2o_automl
title: H2O AutoML
date: "2019-12-22"
image: h2o_autoML_square.jpg
highlight: True

tags:
  - Python
  - Spark
  - H2O

tags_filter:
  - Python
  - Spark

# --------------------------------------------------------------------------------------------------
# Extra info. This will add a button with href to the url
# --------------------------------------------------------------------------------------------------
link: 
  text: Github
  url: https://github.com/villoro/villoro_posts/tree/master/0026-h2o_automl


# --------------------------------------------------------------------------------------------------
# Content
# --------------------------------------------------------------------------------------------------
brief_markdown: |
  TODO

image_head:
  filename: h2o.png
  caption: h2o

use_chartjs: True

content_markdown: |

  ## Table of Contents

  [TOC]

  ## 1. What is H2O

  According to their website, H2O is a fully open source, distributed in-memory machine learning platform with linear scalability.
  H2O supports the most widely used statistical & machine learning algorithms including gradient boosted machines, generalized linear models, deep learning and more.

  It uses MapReduce to break down tasks so that it can send tasts to workers on a cluster.

  H2O also has an AutoML functionality that automatically runs through all the algorithms and their hyperparameters to produce a leaderboard of the best models.

  ## 2. H2O AutoML

  The best way to understand **AutoML** is by showing a practical case.
  As an example we will use the [Higgs Challenge](https://www.kaggle.com/c/higgs-boson/data) data.
  Since the preprocessing is out of the scope of this post we can directly use a copy of the preprocessed data:

  <div class="w3-row w3-center" style="max-width:600px">
    <div class="w3-col m2 w3-margin">
      <a 
        href="https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv"
        class="w3-button v-color-accent w3-round-xxlarge w3-hover-amber"
      >
        Train data
      </a>
    </div>

    <div class="w3-col m2 w3-margin">
      <a
        href="https://s3.amazonaws.com/erin-data/higgs/higgs_test_5k.csv"
        class="w3-button v-color-accent w3-round-xxlarge w3-hover-amber"
      >
        Test data
      </a>
    </div>
  </div>

  ### 2.1. Start H2O session

  This is really straightforward:

  ```python
  import h2o
  from h2o.automl import H2OAutoML, get_leaderboard

  h2o.init()
  ```

  This code will init an H2O session.

  ### 2.2. Get data

  The first step is to load the data.
  Then we will create a list with the names of all feature columns an another for the target.
  The last step is to mark the target as a `factor`. This means setting it as a target.
  All this can be done with:

  ```python
  # Import a sample binary outcome train/test set into H2O
  train = h2o.import_file("https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv")
  test = h2o.import_file("https://s3.amazonaws.com/erin-data/higgs/higgs_test_5k.csv")

  # Identify predictors and response
  x = train.columns
  y = "response"
  x.remove(y)

  # For binary classification, response should be a factor
  train[y] = train[y].asfactor()
  test[y] = test[y].asfactor()
  ```

  > This creates an `H2O` dataframe. You can always transform it to **Pandas** with `x.as_data_frame()`

  ### 2.3. Train

  The first step is to create the `H2OAutoML` object.
  Since we aim to get reproducibility of the example we will set a seed.
  When you do so you also need to specify the maximum number of models to be trained.

  ```python
  aml = H2OAutoML(max_models=20, seed=1, max_runtime_secs=training_minutes*60)
  ```

  I also find it useful to limit the total amount of time that **AutoML** can spend on training.

  Once the **AutoML** object is declared to train you only need to pass the `training_frame` and the names of both **features** and **targets**.

  ```python
  aml.train(x=x, y=y, training_frame=train)
  ```

  ### 2.3. Check results

  You can see the results with `lb = aml.leaderboard`.
  However it is more useful to see all possible information with:

  ```python
  # Optionally add extra model information to the leaderboard
  lb = get_leaderboard(aml, extra_columns='ALL')

  # Print all rows (instead of default 10 rows)
  lb.head(rows=lb.nrows)
  ```

  The results from the example are:

  <table class="v-table" align="center">
    <tr class="v-table-center">
      <th class="v-table-header">model_id</th>
      <th class="v-table-header">auc</th>
      <th class="v-table-header">logloss</th>
      <th class="v-table-header">aucpr</th>
      <th class="v-table-header">mean_per_class_error</th>
    </tr>
    <tr>
      <td>StackedEnsemble_AllModels_AutoML</td>
      <td>0.786427</td>
      <td>0.555344</td>
      <td>0.803633</td>
      <td>0.319589</td>
    </tr>
    <tr>
      <td>StackedEnsemble_BestOfFamily_AutoML</td>
      <td>0.783762</td>
      <td>0.557932</td>
      <td>0.800806</td>
      <td>0.33061</td>
    </tr>
    <tr>
      <td>GBM_5_AutoML</td>
      <td>0.780862</td>
      <td>0.559708</td>
      <td>0.79783</td>
      <td>0.325399</td>
    </tr>
    <tr>
      <td>GBM_1_AutoML</td>
      <td>0.778997</td>
      <td>0.56159</td>
      <td>0.796523</td>
      <td>0.326697</td>
    </tr>
    <tr>
      <td>GBM_grid__1_AutoML_model_2</td>
      <td>0.778615</td>
      <td>0.591319</td>
      <td>0.795194</td>
      <td>0.34516</td>
    </tr>
    <tr>
      <td>GBM_2_AutoML</td>
      <td>0.778338</td>
      <td>0.561527</td>
      <td>0.79632</td>
      <td>0.329805</td>
    </tr>
    <tr>
      <td>GBM_3_AutoML</td>
      <td>0.776389</td>
      <td>0.563906</td>
      <td>0.793284</td>
      <td>0.328065</td>
    </tr>
    <tr>
      <td>GBM_4_AutoML</td>
      <td>0.770758</td>
      <td>0.570912</td>
      <td>0.790371</td>
      <td>0.353743</td>
    </tr>
    <tr>
      <td>DRF_1_AutoML</td>
      <td>0.765151</td>
      <td>0.580246</td>
      <td>0.783285</td>
      <td>0.340491</td>
    </tr>
    <tr>
      <td>XRT_1_AutoML</td>
      <td>0.765134</td>
      <td>0.582172</td>
      <td>0.783059</td>
      <td>0.349171</td>
    </tr>
  </table>

  ## 3. H2O vs Manual ML

  In order to compare the results of **H2O AutoML** I trained a Support Vector Classifier (SVC) and a Random Forest Classifier (RFC).
  Then I also did a GridSearchCV with the RandomForest to get some numbers.

  The parameters for the GridSearchCV are:
  ```yaml
  {'n_estimators': [50, 100, 150, 200], 'max_features': ['auto', 'sqrt', 'log2']}
  ```

  And the 3 metrics analized are:

  * Training time
  * Prediction time
  * [AUC](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)

  Here you can see how all models perform:
  <canvas id="auc_predict_time" style="width:100%;height:500px;"></canvas>

  **AutoML** models are usally faster for predict and some of them perform better than the manual ones.

  <canvas id="auc_train_time" style="width:100%;height:500px;"></canvas>

  The training time with **AutoML** was lower than sklearn while having equal or better results.

  ## 4. Sparkling water

  Even though **H2O** uses MapReduce it might be useful to integrate it with spark.
  To do so you only need to use **Sparkling water**.
  This will help distribute tasks to workers using **Spark**.

  <div class="w3-center">
    <img 
      src="/static/images/posts/h2o_sparkling_water_architecture.png"
      alt="h2o_sparkling_water_architecture"
      class="w3-image w3-padding"
      style="max-height: 500px;"
    />
  </div>

  To install it read the official documentation.

  Once it is installed you only need to replace the **H2O** initalization (`h2o.init()`) for:

  ```python
  from pyspark.sql import SparkSession
  from pysparkling import H2OContext

  spark = SparkSession.builder.appName("h2o_auto_ml").getOrCreate()
  hc = H2OContext.getOrCreate(spark)
  ```


scripts: |
  <script>
    var xAxes = [{
      scaleLabel: {
        display: true,
        labelString: 'AUC'
      }
    }]

    var tooltips = {
      callbacks: {
        label: function(tooltipItem, data) {
          var index = tooltipItem.index;
          var datasetIndex = tooltipItem.datasetIndex;
          
          var dataset = data.datasets[datasetIndex];
          var item = dataset.data[index];

          return item.name + " (AUC: " + item.x + ", time: " + item.y + "s)";
        }
      }
    }

    var data_auc_predict_time = {
      datasets: [
        {
          label: 'AutoML_2min',
          backgroundColor: "#2196F3", 
          data: [
            {'x': 0.6867, 'y': 0.668, 'name': 'StackedEnsemble_AllModels_AutoML'},
            {'x': 0.6797, 'y': 0.4454, 'name': 'StackedEnsemble_BestOfFamily_AutoML'},
            {'x': 0.703, 'y': 0.0952, 'name': 'GBM_5_AutoML'},
            {'x': 0.7066, 'y': 0.084, 'name': 'GBM_1_AutoML'},
            {'x': 0.6981, 'y': 0.1359, 'name': 'GBM_grid__1_AutoML_model_2'},
            {'x': 0.7072, 'y': 0.0852, 'name': 'GBM_2_AutoML'},
            {'x': 0.7004, 'y': 0.0868, 'name': 'GBM_3_AutoML'},
            {'x': 0.7138, 'y': 0.089, 'name': 'GBM_4_AutoML'},
            {'x': 0.6349, 'y': 0.1039, 'name': 'DRF_1_AutoML'},
            {'x': 0.6328, 'y': 0.1095, 'name': 'XRT_1_AutoML'},
            {'x': 0.665, 'y': 0.0504, 'name': 'GBM_grid__1_AutoML_model_3'},
            {'x': 0.6512, 'y': 0.0547, 'name': 'DeepLearning_1_AutoML'},
            {'x': 0.6134, 'y': 0.0458, 'name': 'GLM_1_AutoML'}
          ],
        },
        {
          label: 'sklearn',
          backgroundColor: "#FF9800", 
          data: [
            {'x': 0.642, 'y': 1.5882, 'name': 'SVC_sklearn'},
            {'x': 0.7015, 'y': 0.0974, 'name': 'RFC_sklearn'},
            {'x': 0.7081, 'y': 0.203, 'name': 'RFC_GS_sklearn'}
          ],
        },
      ]
    }

    var auc_predict_time = new Chart("auc_predict_time", {
      type: 'scatter',
      data: data_auc_predict_time,
      options: {
        title: {
          display: true,
          text: 'AUC vs predict time [s]'
        },
        scales: {
          xAxes: xAxes,
          yAxes: [{
            scaleLabel: {
              display: true,
              labelString: 'Predict time [seconds]'
            }
          }]
        },
        tooltips: tooltips
      }
    });

    var data_auc_train_time = {
      datasets: [
        {
          label: 'AutoML_2min',
          backgroundColor: "#2196F3", 
          data: [
            {'x': 0.6867, 'y': 1.393, 'name': 'StackedEnsemble_AllModels_AutoML'},
            {'x': 0.6797, 'y': 0.743, 'name': 'StackedEnsemble_BestOfFamily_AutoML'},
            {'x': 0.703, 'y': 1.092, 'name': 'GBM_5_AutoML'},
            {'x': 0.7066, 'y': 0.804, 'name': 'GBM_1_AutoML'},
            {'x': 0.6981, 'y': 2.936, 'name': 'GBM_grid__1_AutoML_model_2'},
            {'x': 0.7072, 'y': 0.813, 'name': 'GBM_2_AutoML'},
            {'x': 0.7004, 'y': 0.895, 'name': 'GBM_3_AutoML'},
            {'x': 0.7138, 'y': 1.165, 'name': 'GBM_4_AutoML'},
            {'x': 0.6349, 'y': 1.815, 'name': 'DRF_1_AutoML'},
            {'x': 0.6328, 'y': 2.541, 'name': 'XRT_1_AutoML'},
            {'x': 0.665, 'y': 0.166, 'name': 'GBM_grid__1_AutoML_model_3'},
            {'x': 0.6512, 'y': 0.893, 'name': 'DeepLearning_1_AutoML'},
            {'x': 0.6134, 'y': 0.235, 'name': 'GLM_1_AutoML'}
          ],
        },
        {
          label: 'sklearn',
          backgroundColor: "#FF9800", 
          data: [
            {'x': 0.642, 'y': 5.1965, 'name': 'SVC_sklearn'},
            {'x': 0.7015, 'y': 3.6673, 'name': 'RFC_sklearn'},
            {'x': 0.7081, 'y': 60.3997, 'name': 'RFC_GS_sklearn'}
          ],
        },
      ]
    }

    var auc_train_time = new Chart("auc_train_time", {
      type: 'scatter',
      data: data_auc_train_time,
      options: {
        title: {
          display: true,
          text: 'AUC vs training time [s]'
        },
        scales: {
          xAxes: xAxes,
          yAxes: [{
            scaleLabel: {
              display: true,
              labelString: 'Training time [seconds]'
            }
          }]
        },
        tooltips: tooltips
      }
    });
  </script>