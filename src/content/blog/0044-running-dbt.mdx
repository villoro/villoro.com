---
slug: running-dbt-with-aws-ecs
title: Running DBT with AWS ECS
meta_title: Running DBT with AWS ECS
description: Guide to setting up and running DBT on AWS ECS (Fargate), covering Dockerization, package handling, Docker images, integration with Prefect, exporting results, and CI with pre-commit.
date: 2024-05-09
image: "/images/blog/0044-dbt.png"
image_dark: "/images/blog/0044-dbt-dark.png"
tags: ["SQL", "DE", "AWS"]
draft: false
---

## Table of Contents

[TOC]

## 0. Motivation

<FancyLink linkText="DBT" url="https://www.getdbt.com/"/> serves as a gateway for SQL practitioners to engage more deeply in data engineering tasks.
While DBT Cloud offers a managed solution, budget constraints may lead to exploring self-hosting options.
Leveraging <FancyLink linkText="AWS ECS" url="https://aws.amazon.com/en/ecs/" company="aws"/> (Fargate) aligns with our existing AWS infrastructure and offers cost-effective scalability.
By self-hosting DBT on AWS ECS (Fargate), we gain better control and observability over our data transformation processes.

When setting this up I got a lot of information and direction from those posts:

* <FancyLink linkText="Running dbt-core in production on AWS using ECS Fargate and Airflow" url="https://data-dive.com/deploy-dbt-on-aws-using-ecs-fargate-with-airflow-scheduling/" company="default" dark="true"/>
* <FancyLink linkText="Deploying and Running dbt on AWS Fargate" url="https://medium.com/hashmapinc/deploying-and-running-dbt-on-aws-fargate-872db84065e4" dark="true"/>

However I still thing I have multiple improvements that I wanted to share such as how to handle it with `poetry`, integration with `prefect`, exporting `DBT` run results among other things.
So this is why I'm documenting everything.

## 1. The components

In our setup, we combine `DBT`, `Docker`, and `AWS ECS (Fargate)` to streamline the execution of data transformation pipelines:

* **DBT** enables SQL practitioners to efficiently transform and model data.
* **Docker** containerizes the necessary Python code for DBT, ensuring consistency and reproducibility.
* **AWS ECS (Fargate)** executes Docker containers, providing a lightweight and serverless solution for running DBT.

<Notice type="warning">
  Teams using AWS EKS can replicate this setup with minimal effort, leveraging existing Kubernetes clusters.
</Notice>

There are also some components that can be easily swapped by similar tools. They are:

* **Prefect** as the orchestrator that runs scheduled jobs and provides observability
* **Athena** as the query engine
* **Github actions** for both Continuous Integration (CI) and Continous Delivery (CD)
* **Glue** as the data catalog

Here you can see a diagram of the overall architecture:

![Backfill missing data](../../images/posts/2024/0044-dbt-architecture.jpg)

## 2. Options for handling packages in docker

From the begining I wanted to have a simple `Docker` image that it is independant from the `DBT` code.
This way changes to the `DBT` itself won't imply creating a new image and uploading it to `ECR`.

So the solution is to create two `.tar.gz` files:

* `package.tar.gz` file that contains all `DBT` code
* `venv.tar.gz`: this is the virtual environment so that it does not need to install packages at runtime.

Both files are downloaded at runtime and are versioned so that we can easily use older versions if needed.
In order to decide which version to download and use I export the `pyproject.toml` which contains the `version` number.

You can read more about it in <FancyLink linkText="Deploying and Running dbt on AWS Fargate" url="https://medium.com/hashmapinc/deploying-and-running-dbt-on-aws-fargate-872db84065e4" dark="true"/> (section `Docker Image`).

## 3. Docker images

With the described approach we have 3 docker images:

* **Venv**: This is the image used for creating the `venv` that contains the `DBT` code
* **ECS**: This is the one that will be run in `ECS (Fargate)`
* **Base**: This image contains the shared code and packages between the other two dockers

How to upload those images to `ECR` is out of the scope.
In general I follow somothing similar to what is described in <FancyLink linkText="How to Build and Push Docker Images to AWS ECR" url="https://www.freecodecamp.org/news/build-and-push-docker-images-to-aws-ecr/"/>

### 3.1. Docker.base

This base docker image simply installs python and some basic python packages.

<TerminalOutput color="stone">
  .github/docker/Dockerfile.base
</TerminalOutput>
```docker
# Top level build args
ARG BUILD_FOR=linux/amd64

FROM --platform=${BUILD_FOR} python:3.10.7-slim-bullseye as base

# Env vars + Args
ARG POETRY_VERSION=1.6.1 # Should match CI config 
ENV PYTHONIOENCODING=utf-8
ENV LANG=C.UTF-8

WORKDIR /usr/app/dbt/

# System setup
RUN apt-get update \
  && apt-get dist-upgrade -y \
  && apt-get install -y --no-install-recommends \
    git \
    ssh-client \
    software-properties-common \
    make \
    build-essential \
    ca-certificates \
    libpq-dev \
  && apt-get clean \
  && rm -rf \
    /var/lib/apt/lists/* \
    /tmp/* \
    /var/tmp/*

# Update and install python packages
RUN python -m pip install --upgrade \
  pip \
  setuptools \
  wheel \
  poetry==${POETRY_VERSION} \
  boto3 \
  loguru \
  toml \
  --no-cache-dir
```

### 3.2. Docker.venv

This image creates a virtual environment where it installs all DBT code.
Then it packages that virutal environment into a `tar.gz` file.

<TerminalOutput color="stone">
  .github/docker/Dockerfile.venv
</TerminalOutput>
```docker
FROM dbt/base as base

ARG PACKAGE_VERSION

# Copy the scripts needed for setting up the environment (and yes, all of them are needed...)
COPY pyproject.toml .
COPY poetry.lock .
COPY README.md .
COPY dbt_src dbt_src

# Set up poetry and install dependencies
RUN poetry config virtualenvs.in-project true && \
    poetry install

# Install 'dbt_src' as a package
RUN poetry build && \
    poetry run pip install dist/*.whl

# Create a '.tar.gz' with the virtual environment
RUN poetry run venv-pack -o dist/venv_${PACKAGE_VERSION}.tar.gz

# Export the '.tar.gz'
FROM scratch AS export
COPY --from=base /usr/app/dbt/dist/*.tar.gz /dist/
```
<Notice type="warning">
  We **do need** to copy `pyproject.toml`, `poetry.lock` and `README.md` for it to work.
</Notice>

<Notice type="info" className="mt-6">
  The `dbt_src` is the folder that contains the actual `DBT` code.
  That means the `dbt_project.yml`; the `models`, `tests`, `macros` folders etc.
</Notice>

### 3.3. Docker.ecs

This is the image that will be run in `Fargate`.
This is where I set up the connection to `prefect` (more info at <FancyLink linkText="Connect Docker | Setting Up and Deploying Prefect Server" url="https://villoro.com/blog/prefect-server-setup-configuration-deployment#31-connect-docker/" dark="true"/>).
Also I export all the needed `scripts` that will later be used to download all `DBT` code.

<TerminalOutput color="stone">
  .github/docker/Dockerfile.ecs
</TerminalOutput>
```docker
FROM dbt/base

# Instll prefect
ARG PREFECT_VERSION=2.14.20 # Should match poetry.lock
RUN python -m pip install prefect==${PREFECT_VERSION}

# Connect to prefect
ARG PREFECT_API_URL # This should end with '/api'
ENV PREFECT_API_URL=${PREFECT_API_URL}

# Copy the scripts needed for setting up the environment
COPY scripts .

# This will download the needed files + install poetry environment + execute all DBT commands by default
ENTRYPOINT ["/bin/bash", "./setup_and_run_dbt.sh"]
```

## 4. Continuous Delivery (CD)

As mentioned before there are two `tar.gz` files.
One with the virtual environment (`venv`) and another with the code (`dist`).
The goal of `CD` is to create both and upload them to S3 together with the `pyproject.toml` and `profiles.yml` file.

### 4.1. CD Pacakges github action

```yaml
name: CD Packages

on:
  push:
    branches:
      - main
    paths:
      - dbt_src/**
      - profiles/**
      - pyproject.toml
      - poetry.lock

jobs:
  deploy_package_pro:
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read
    steps:
    - uses: actions/checkout@v4
    - uses: actions/setup-python@v4

    - name: Install python dependencies
      run: pip install boto3 click loguru toml

    - name: Get current version
      run: python .github/scripts/get_version.py --name=current

    - name: Build package
      run: |
        docker build -f .github/docker/Dockerfile.base -t dbt/base .
        docker build -f .github/docker/Dockerfile.venv --output . . --build-arg PACKAGE_VERSION=$VERSION_CURRENT

    - name: Configure DevOps AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ vars.AWS_ROLE_DATA }}
        aws-region: eu-west-1
    
    - name: Configure DatalakePro AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: xxx # Set the role ARN here
        aws-region: eu-west-1
        role-chaining: true
  
    - name: Upload files to S3 PRO
      run: python .github/scripts/upload_all.py --env pro
```
<Notice type="warning">
  You need to set the `role ARN` (replace `xxx` in the code) needed for uploading the files to S3.
</Notice>

### 4.2. Upload all script

There are multiple ways the documents can be uploaded.
Since I'm proficient with `python` I'm using it but you can take any alternative approach following a similar approach.

<TerminalOutput color="stone">
  .github/scripts/upload_all.py
</TerminalOutput>
```python
import os

import boto3
import click
import utils as u
from loguru import logger


def upload_to_s3(bucket, origin, dest):
    logger.info(f"Uploading from {origin=} to {dest=}")

    s3_client = boto3.client("s3")
    s3_client.upload_file(origin, bucket, dest)


def upload_dbt_package(bucket, s3_dist, s3_venv):
    logger.info(f"Exploring packages to upload")
    for filename in os.listdir("dist"):
        if filename.endswith(".tar.gz"):
            path = s3_venv if filename.startswith("venv") else s3_dist
            upload_to_s3(bucket, f"dist/{filename}", f"{path}/{filename}")


@click.command()
@click.option("--env", default="pro", help="Environment where code will be deployed")
def upload_all(env):
    aws_config = u.get_aws_config()

    env = env.lower()
    logger.info(f"Working on {env=}")
    assert env in aws_config["s3_bucket"].keys()

    logger.info(f"Extractings paths")
    bucket = aws_config["s3_bucket"][env]
    s3_code = aws_config["paths"]["s3_code"]
    s3_dist = aws_config["paths"]["s3_dist"]
    s3_venv = aws_config["paths"]["s3_venv"]

    # Upload poetry config
    upload_to_s3(bucket, "pyproject.toml", f"{s3_code}/pyproject.toml")
    upload_to_s3(bucket, "poetry.lock", f"{s3_code}/poetry.lock")

    # DBT config
    profile = aws_config["filenames"]["profiles_by_env"]
    upload_to_s3(bucket, profile.format(env=env), f"{s3_code}/profiles.yml")
    upload_dbt_package(bucket, s3_dist, s3_venv)


if __name__ == "__main__":
    upload_all()
```

<Notice type="danger" className="mt-6">
  I use the `pyproject.toml` file to store a lot of configuration such as `S3` paths.
  I haven't included a snippet since most data is private but it should be easy to infer.
</Notice>

## 5. Scripts

During runtime we will need to download all files created during the `CD`.
This is done with the following 3 scripts:

* `scripts/download_poetry_config.py`
* `scripts/download_code.py`
* `scripts/setup_and_run_dbt.sh`: Calls the previous scripts, configures the virtual environment and defines the entrypoint

### 5.1. Download poetry config

This script:

1. Downloads the `pyproject.toml` 
2. Downloads the `venv.tar.gz` and extracts it

<TerminalOutput color="stone">
  scripts/download_poetry_config.py
</TerminalOutput>
```python
import tarfile

import boto3
import utils as u
from loguru import logger

# Should match 'pyproject.toml'
S3_BUCKET = {
    "634077897723": "nt-data-engineering-eu-west-1-634077897723-pre",
    "162076964983": "nt-data-engineering-eu-west-1-162076964983-snd",
    "742407267173": "nt-data-engineering-eu-west-1-742407267173-pro",
}
S3_CODE = "dbt/code"


def download_pyproject(bucket):
    pyproject = u.PYPROJECT_FILE
    lock = u.LOCK_FILE
    s3_client = boto3.client("s3")

    logger.info(f"Downloading 's3://{bucket}/{S3_CODE}/{pyproject}'")
    s3_client.download_file(bucket, f"{S3_CODE}/{pyproject}", pyproject)
    logger.info(f"Downloading 's3://{bucket}/{S3_CODE}/{lock}'")
    s3_client.download_file(bucket, f"{S3_CODE}/{lock}", lock)


def venv_download_tar(bucket, s3_venv, filename):
    s3_client = boto3.client("s3")
    logger.info(f"Downloading 's3://{bucket}/{s3_venv}/{filename}'")
    s3_client.download_file(bucket, f"{s3_venv}/{filename}", filename)


def venv_extract_tar(local_venv, filename):
    logger.info(f"Extracting '{filename}' to '{local_venv}'")
    with tarfile.open(filename, "r:gz") as tar:
        tar.extractall(local_venv)


def download_all():
    # Download 'pyproject.toml' asap
    account = u.get_account()
    bucket = S3_BUCKET[account]
    download_pyproject(bucket)

    # Then infer from 'pyproject.toml'
    aws_config = u.get_aws_config()
    logger.info(f"Extractings paths")
    s3_venv = aws_config["paths"]["s3_venv"]
    local_venv = aws_config["paths"]["local_venv"]

    version = u.get_version_from_toml()
    filename = aws_config["filenames"]["venv"].format(version=version)

    # Finally download the tars
    venv_download_tar(bucket, s3_venv, filename)
    venv_extract_tar(local_venv, filename)


if __name__ == "__main__":
    download_all()
```

### 5.2. Download code

This script:

1. Downloads the `profiles.yml`
2. Downloads the `dist.tar.gz` and extracts it

It is a separate script from the previous one because it uses the virtual environment from the previous script.

<TerminalOutput color="stone">
  scripts/download_code.py
</TerminalOutput>
```python
import os
import shutil
import tarfile

import boto3
import toml
import utils as u
from loguru import logger
from prefect import flow
from prefect import get_run_logger
from prefect import tags
from prefect import task


@task(name="dbt.config.dist.download_profiles")
def dist_download_profiles(bucket, s3_code, profiles):
    logger = get_run_logger()
    logger.info(f"Downloading 's3://{bucket}/{s3_code}/{profiles}'")

    s3_client = boto3.client("s3")
    s3_client.download_file(bucket, f"{s3_code}/{profiles}", profiles)


@task(name="dbt.config.dist.download_tar")
def dist_download_tar(bucket, s3_dist, filename):
    logger = get_run_logger()
    logger.info(f"Downloading 's3://{bucket}/{s3_dist}/{filename}'")

    s3_client = boto3.client("s3")
    s3_client.download_file(bucket, f"{s3_dist}/{filename}", filename)


@task(name="dbt.config.dist.extract_tar")
def dist_extract_tar(filename):
    logger = get_run_logger()
    logger.info(f"Extracting '{filename}'")

    with tarfile.open(filename, "r:gz") as tar:
        tar.extractall()


@task(name="dbt.config.dist.move_files")
def move_files(filename, path_temp):
    for filename in os.listdir(path_temp):
        shutil.move(os.path.join(path_temp, filename), filename)


@task(name="dbt.config.dist.clean_files")
def clean_files(filename, path_temp):
    logger = get_run_logger()
    logger.info(f"Removing '{filename}'")
    os.remove(filename)

    path_to_delete = path_temp.split("/")[0]
    logger.info(f"Removing '{path_to_delete}/'")
    shutil.rmtree(path_to_delete)


@flow(name="dbt.config.dist")
def download_dist(version):
    account = u.get_account()
    aws_config = u.get_aws_config()

    logger.info(f"Extractings paths and filenames from '{u.PYPROJECT_FILE}'")
    env = aws_config["accounts"][account]
    bucket = aws_config["s3_bucket"][env]
    s3_code = aws_config["paths"]["s3_code"]
    s3_dist = aws_config["paths"]["s3_dist"]
    profiles = aws_config["filenames"]["profiles"]
    dist = aws_config["filenames"]["dist"].format(version=version)
    dist_temp = aws_config["filenames"]["dist_temp"].format(version=version)

    dist_download_profiles(bucket, s3_code, profiles)
    dist_download_tar(bucket, s3_dist, dist)
    dist_extract_tar(dist)
    move_files(dist, dist_temp)
    clean_files(dist, dist_temp)


def main():
    u.ping_prefect()

    version = u.get_version_from_toml()
    run_tags = ["type:dbt.config", f"version:{version}"]

    with tags(*run_tags):
        logger.info(f"Downloading code with {run_tags=}")
        logger.info("Running first prefect task inside docker")
        download_dist(version)


if __name__ == "__main__":
    main()
```

### 5.3. Setup and run DBT

This script:

* Calls the other scripts
* Configures the virtual environment
* Calls the real `entrypoint` of the docker (more on that on the next section)

<TerminalOutput color="stone">
  scripts/setup_and_run_dbt.sh
</TerminalOutput>
```sh
echo "1. Downloading Poetry config"
python download_poetry_config.py

echo "2. Setting up Poetry"
poetry config virtualenvs.in-project true

echo "3. Downloading all Poetry code"
poetry run python download_code.py

echo 5. Running all DBT commands. Using args="$@"
poetry run python run.py "$@"
```

<Notice type="danger" className="mt-6">
  The `"$@"` is used to pass all arguments form the docker to the `run.py` script
</Notice>

## 6. Running DBT

<FancyLink linkText="Programmatic invocations" url="https://docs.getdbt.com/reference/programmatic-invocations"/>

## 6. Integrating with Prefect

### 6.1. Tasks

### 6.2. Adding logs

## 7. Exporting DBT results
