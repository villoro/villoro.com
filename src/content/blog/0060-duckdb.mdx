---
slug: duckdb-fast-analytics
title: DuckDB and Beyond
meta_title: DuckDB and Beyond - Fast Analytics on Your Laptop
description: Discover DuckDB, the “SQLite for analytics”. Learn why it’s so fast, how its SQL superpowers extend beyond standard SQL, and how to use it for local exploration, data movement, CI pipelines with DBT, and even experiment with DuckLake.
date: 2025-08-24
image: /images/blog/0060-duckdb.jpg
category: tools
tags: [Python, SQL, DE]
draft: false
---

## 1. What is DuckDB?

<FancyLink linkText="DuckDB" url="https://duckdb.org/" dark="true"/> is often described as **“SQLite for analytics”**. Like SQLite, it’s an embedded database engine that runs directly on your machine, but it’s designed for analytical workloads rather than transactional ones. This makes it an incredibly convenient tool for anyone working with data: no server to install, no cluster to spin up—just query your data where it is.

<FancyLink linkText="DuckDB" url="https://duckdb.org/" dark="true"/> sits in the sweet spot between lightweight tools like Pandas and heavyweight distributed systems like Spark. It’s fast, simple, and surprisingly powerful for working with large datasets on a single machine.

## 2. Why DuckDB is Faster Than You Expect

DuckDB isn’t just “fast for small data.” Its execution model lets a single machine chew through datasets that used to feel like cluster territory.

<Notice type="success">
  In independent and first‑party benchmarks, DuckDB routinely handles **hundreds of millions to billions of rows** on commodity hardware and often beats heavier frameworks on local workloads—see the sources in each subsection below.
</Notice>

### 2.1. How it achieves speed (in short)

DuckDB’s engine is **columnar & vectorized** and aggressively **multithreaded**, with a push‑based pipeline and out‑of‑core operators when memory is tight. If you want the gory details (and tuning tips), the official performance guide is a great, concise tour: <FancyLink linkText="DuckDB Performance Guide" url="https://duckdb.org/docs/stable/guides/performance/overview.html" dark="true"/>.

### 2.2. What the numbers say (real‑world speed)

* **Against dataframes on a laptop (10M rows generation/joins):** On a MacBook Pro M2 (16 GB), DuckDB completed the task in **3.84 s**, vs **5.77 s** for Polars (\~**1.5×** slower) and **19.57 s** for Pandas (\~**5.1×** slower). Source: <FancyLink linkText="DuckDB, Polars and Pandas performance comparison" url="https://github.com/prrao87/duckdb-study" dark="true"/>.

* **Single‑node pipeline vs Spark (40 GiB Parquet, 52 files, 94 cols, c2d‑standard‑16 16‑core/64 GB):** DuckDB finished the group‑by and wrote a single Parquet output in **3.99 s**. chDB took **9.30 s** and Spark **16.39 s** under the same setup. Source: <FancyLink linkText="Small‑Scale Data Pipeline: DuckDB vs. chDB vs. Spark" url="https://www.digitalturbine.com/blog/small-scale-data-pipeline-spark-vs-chdb-vs-duckdb" dark="true" company="default"/>.

* **Parquet scan & simple aggregation (M1 Mac):** Copying a directory of Parquet logs to a single Parquet file took **\~4.91 s**; filtering & grouping returned in **\~0.073 s**. Source: <FancyLink linkText="How fast DuckDB can query Parquet files?" url="https://dev.l1x.be/posts/2023/03/26/how-fast-duckdb-can-query-parquet-files/" dark="true" company="default"/>.

### 2.3. Surprising scale on a single machine (larger‑than‑memory)

DuckDB’s **external (out‑of‑core) aggregation** lets you process data far bigger than RAM by spilling intelligently and recomputing pointers instead of (de)serializing.

* **1 billion rows (\\~50 GB) on a 16 GB laptop:** Using the H2O.ai `G1` dataset, DuckDB completed all 10 aggregation queries on a 2020 MacBook Pro (16 GB). On an AWS `c6id.metal` (64 cores/128 threads, 256 GB), the hardest query (Q10) ran in **8.58 s**; on the laptop it took **\~264 s** (still completes). For reference, **Spark took 603.05 s** on the beefy `c6id.metal` for Q10. Source: <FancyLink linkText="External Aggregation in DuckDB (benchmarks & method)" url="https://duckdb.org/2024/03/29/external-aggregation.html" dark="true"/>.

* **Scale gains on the same hardware:** Over the 2021→2024 period, DuckDB pushed group‑by and join scale to **\~10× larger** datasets on the same M1 MacBook (16 GB), thanks to larger‑than‑memory operators and memory‑manager upgrades. Source: <FancyLink linkText="Benchmarking Ourselves over Time" url="https://duckdb.org/2024/06/26/benchmarks-over-time.html" dark="true"/>.

### 2.4. It keeps getting faster (measured over time)

On a standard benchmark suite (5 GB scale for mixed workloads; 50 GB for group‑bys/joins) run on an M1 MacBook Pro (16 GB), total runtime fell from **\~500 s (DuckDB 0.2.7, Jun 2021)** to **< 35 s (DuckDB 1.0.0, Jun 2024)**—a **\~14× improvement**. Highlights on the same machine: **aggregations \~12×**, **joins \~4×**, **window functions \~25×**, **CSV imports \~3×**, **Parquet exports \~4–5×**. Source: <FancyLink linkText="Benchmarking Ourselves over Time" url="https://duckdb.org/2024/06/26/benchmarks-over-time.html" dark="true"/>.

> **Proposed plot placement:** Right below this subsection, insert a simple line chart of *Total benchmark runtime (s)* per DuckDB version (0.2.7 → 1.0.0) from the article above to visualize the \~14× speedup.

## 3. DuckDB SQL Superpowers

DuckDB isn’t just about speed—it also provides a SQL dialect full of modern features.

### 3.1. Extended SQL syntax

It comes with quality-of-life improvements like easy joins, better casting, and functions you wish existed in standard SQL.

### 3.2. Built-in functions for analytics

DuckDB includes **window functions, regex support, JSON handling, and the ability to read/write Parquet and CSV directly**.

### 3.3. Interoperability with Python/R

DuckDB integrates tightly with Python and R. You can query directly against DataFrames:

```python
import duckdb
import pandas as pd

# Create a DataFrame
my_df = pd.DataFrame({"id": [1, 2, 3], "value": [10, 20, 30]})

# Query the DataFrame with DuckDB
df = duckdb.query("SELECT * FROM my_df WHERE value > 15").to_df()
print(df)
```

This makes it perfect for combining the productivity of Pandas with the performance of SQL.

## 4. Practical Examples

DuckDB shines when put to work in everyday scenarios.

### 4.1. Exploring data locally

Need to inspect a Parquet or CSV file? With DuckDB you can query it directly:

```bash
duckdb -ui
```

This launches an interactive shell where you can run SQL queries instantly—no setup required.

```sql
SELECT country, city, COUNT(*)
FROM 'data/events.parquet'
GROUP BY ALL
ORDER BY 3 DESC;
```

### 4.2. Acting as a data mover

DuckDB can serve as a **bridge** between systems. For example, you can read from Postgres or MySQL and export to Parquet, or the other way around.

```sql
COPY (SELECT * FROM postgres_scan('dbname=mydb', 'SELECT * FROM users')) 
TO 'users.parquet' (FORMAT PARQUET);
```

This makes it a handy connector when you just want to move data around efficiently.

### 4.3. Using DuckDB in CI pipelines with DBT

DuckDB is also a fantastic choice for testing transformations locally or in CI. You can run DBT models against DuckDB to validate logic without needing cloud infrastructure. (I’ve written a dedicated post on this—check it out!)

## 5. DuckLake: A Young but Promising Idea

DuckDB is already powerful, but the community is pushing it further.

### 5.1. The concept of bringing data lakehouse features into DuckDB

**DuckLake** is an experimental layer that adds data lakehouse capabilities on top of DuckDB—things like ACID transactions and table formats.

### 5.2. Current state and potential

It’s still early, but the vision is exciting: combining DuckDB’s simplicity with the robustness of lakehouse technologies. If it matures, DuckLake could make DuckDB even more relevant in production scenarios.

## 6. Closing Thoughts

DuckDB has quickly gone from a niche project to a serious tool in the data engineer’s toolbox.

### 6.1. Where DuckDB shines today

* Local analytics at lightning speed
* Simple exploration of large Parquet/CSV files
* Acting as glue between different databases
* Lightweight CI pipelines with DBT

### 6.2. Why it’s worth experimenting with DuckLake

Even though DuckLake is still young, its vision aligns with the future of data: fast, simple, lakehouse-native.

### 6.3. Final encouragement for readers to try it

If you haven’t already, give DuckDB a try. Chances are, it’ll surprise you with how much you can accomplish with just your laptop.
