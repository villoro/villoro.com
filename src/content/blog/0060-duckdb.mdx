---
slug: duckdb-fast-analytics
title: DuckDB and Beyond
meta_title: DuckDB and Beyond - Fast Analytics on Your Laptop
description: Discover DuckDB, the “SQLite for analytics”. Learn why it’s so fast, how its SQL superpowers extend beyond standard SQL, and how to use it for local exploration, data movement, CI pipelines with DBT, and even experiment with DuckLake.
date: 2025-08-24
image: /images/blog/0060-duckdb.jpg
category: tools
tags: [Python, SQL, DE]
draft: false
---

## 1. What is DuckDB?

<FancyLink linkText="DuckDB" url="https://duckdb.org/" dark="true"/> is often described as **“SQLite for analytics”**. Like SQLite, it’s an embedded database engine that runs directly on your machine, but it’s designed for analytical workloads rather than transactional ones. This makes it an incredibly convenient tool for anyone working with data: no server to install, no cluster to spin up—just query your data where it is.

<FancyLink linkText="DuckDB" url="https://duckdb.org/" dark="true"/> sits in the sweet spot between lightweight tools like Pandas and heavyweight distributed systems like Spark. It’s fast, simple, and surprisingly powerful for working with large datasets on a single machine.

## 2. Why DuckDB is Faster Than You Expect

At first glance, you might assume DuckDB is only good for medium-sized datasets—something that fits comfortably in memory. In reality, DuckDB can push the limits of what you think is possible on a single machine. 

<Notice type="success">
  Benchmarks consistently show it handling **hundreds of millions to billions of rows** interactively, often outperforming both Pandas and even distributed systems in local workloads.
</Notice>

### 2.1. In-memory columnar engine

DuckDB uses a **vectorized, columnar format** for storage and execution:

* Scans and aggregations are dramatically faster since only the relevant columns are read.
* Compression is efficient: Parquet files in the **tens of gigabytes** can often be scanned on a laptop in seconds.
* Internal benchmarks show DuckDB can process **\~1–5 GB/s of columnar data** depending on hardware.

Compared to Pandas row-oriented in-memory storage, DuckDB scales far better. For example, on a 10M row dataset, DuckDB was about **5× faster than Pandas** (and \~1.5× faster than Polars) under identical conditions (Source: <FancyLink linkText="DuckDB, Polars and Pandas performance comparison" url="https://github.com/prrao87/duckdb-study" dark="true"/>).

### 2.2. Efficient query execution

DuckDB’s **vectorized execution model** processes data in chunks of \~1000 rows at a time, optimized for CPU caches and SIMD instructions. This avoids Python overhead and makes it highly competitive:

* A **group by on 100M rows** can run in **under 10 seconds** on a modern laptop (<FancyLink linkText="DuckDB Benchmarks" url="https://duckdb.org/2024/06/26/benchmarks-over-time.html" dark="true"/>).
* Joins across **two 1 GB Parquet files** complete in just a few seconds.
* Parallel execution scales well: an 8-core machine often achieves **5–6× faster queries** compared to single-thread.

Benchmarks show DuckDB beating Spark on some medium-scale jobs: e.g. a 40 GB aggregation took DuckDB **\~4 s vs. Spark’s 16 s** on the same 16-core machine (Source:
<FancyLink linkText="Small-Scale Data Pipeline: DuckDB vs. chDB vs. Spark" url="https://www.digitalturbine.com/blog/small-scale-data-pipeline-spark-vs-chdb-vs-duckdb" dark="true" company="default"/>).

### 2.3. Surprising scale on a single machine

DuckDB is not limited to RAM size. Its query engine streams data off disk, enabling queries larger than memory:

* On a MacBook Pro (M1, 16 GB RAM), DuckDB successfully aggregated **1 billion rows (\~50 GB)**, thanks to its out-of-core group-by algorithm ([DuckDB Labs](https://duckdb.org/2024/06/26/benchmarks-over-time.html)).
* In production tests, a **40 GB Parquet dataset** was aggregated in seconds on a 16-core VM with 64 GB RAM ([Querify Labs](https://www.querifylabs.com/blog/duckdb-vs-spark-parquet)).
* A real ETL pipeline processed **\~100 GB of raw CSV (897M rows)** on a 4-core, 32 GB RAM machine in about **26 minutes**, costing only \~\$0.15 in cloud compute ([Dagster case study](https://dagster.io/blog/duckdb-dagster-pipeline)).
* On Microsoft Fabric, DuckDB remained competitive against Spark up to **700 GB of data** on one node; it only faltered at the **1 TB scale** due to hardware limits ([Microsoft Fabric blog](https://techcommunity.microsoft.com/t5/fasttrack-for-azure/duckdb-on-fabric-benchmarks/ba-p/4176372)).

### 2.4. Performance improvements over time

Another reason DuckDB feels “too fast to be true” is that it’s getting faster with every release. In DuckDB’s own [Benchmarks Over Time (June 2024)](https://duckdb.org/2024/06/26/benchmarks-over-time.html), the total runtime of their standard suite dropped from **\~500 seconds in 2021 to \~35 seconds in 2024** — a **14× improvement**.

Some highlights:

* **Aggregations:** \~12× faster.
* **Joins:** \~4× faster.
* **Window functions:** \~25× faster.
* **CSV imports:** \~3× faster.
* **Parquet exports:** \~4–5× faster.

> **Proposed plot:** This would be a great place to insert a simple line chart showing DuckDB’s benchmark total runtime (seconds) over time (2021 → 2024), highlighting the \~14× speedup. It would visually reinforce the steady gains across versions.

DuckDB’s continuous improvements, combined with its efficient design, explain why it often outperforms expectations on a single machine.

## 3. DuckDB SQL Superpowers

DuckDB isn’t just about speed—it also provides a SQL dialect full of modern features.

### 3.1. Extended SQL syntax

It comes with quality-of-life improvements like easy joins, better casting, and functions you wish existed in standard SQL.

### 3.2. Built-in functions for analytics

DuckDB includes **window functions, regex support, JSON handling, and the ability to read/write Parquet and CSV directly**.

### 3.3. Interoperability with Python/R

DuckDB integrates tightly with Python and R. You can query directly against DataFrames:

```python
import duckdb
import pandas as pd

# Create a DataFrame
my_df = pd.DataFrame({"id": [1, 2, 3], "value": [10, 20, 30]})

# Query the DataFrame with DuckDB
df = duckdb.query("SELECT * FROM my_df WHERE value > 15").to_df()
print(df)
```

This makes it perfect for combining the productivity of Pandas with the performance of SQL.

## 4. Practical Examples

DuckDB shines when put to work in everyday scenarios.

### 4.1. Exploring data locally

Need to inspect a Parquet or CSV file? With DuckDB you can query it directly:

```bash
duckdb -ui
```

This launches an interactive shell where you can run SQL queries instantly—no setup required.

```sql
SELECT country, city, COUNT(*)
FROM 'data/events.parquet'
GROUP BY ALL
ORDER BY 3 DESC;
```

### 4.2. Acting as a data mover

DuckDB can serve as a **bridge** between systems. For example, you can read from Postgres or MySQL and export to Parquet, or the other way around.

```sql
COPY (SELECT * FROM postgres_scan('dbname=mydb', 'SELECT * FROM users')) 
TO 'users.parquet' (FORMAT PARQUET);
```

This makes it a handy connector when you just want to move data around efficiently.

### 4.3. Using DuckDB in CI pipelines with DBT

DuckDB is also a fantastic choice for testing transformations locally or in CI. You can run DBT models against DuckDB to validate logic without needing cloud infrastructure. (I’ve written a dedicated post on this—check it out!)

## 5. DuckLake: A Young but Promising Idea

DuckDB is already powerful, but the community is pushing it further.

### 5.1. The concept of bringing data lakehouse features into DuckDB

**DuckLake** is an experimental layer that adds data lakehouse capabilities on top of DuckDB—things like ACID transactions and table formats.

### 5.2. Current state and potential

It’s still early, but the vision is exciting: combining DuckDB’s simplicity with the robustness of lakehouse technologies. If it matures, DuckLake could make DuckDB even more relevant in production scenarios.

## 6. Closing Thoughts

DuckDB has quickly gone from a niche project to a serious tool in the data engineer’s toolbox.

### 6.1. Where DuckDB shines today

* Local analytics at lightning speed
* Simple exploration of large Parquet/CSV files
* Acting as glue between different databases
* Lightweight CI pipelines with DBT

### 6.2. Why it’s worth experimenting with DuckLake

Even though DuckLake is still young, its vision aligns with the future of data: fast, simple, lakehouse-native.

### 6.3. Final encouragement for readers to try it

If you haven’t already, give DuckDB a try. Chances are, it’ll surprise you with how much you can accomplish with just your laptop.
