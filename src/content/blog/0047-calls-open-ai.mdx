---
slug: process-calls-open-ai
title: Process Calls with Open AI
meta_title: Process Calls with Open AI
description: xx
date: 2024-06-13
image: "/images/blog/0047-process-calls-openai.jpg"
tags: ["Python", "DE", "AI"]
draft: false
---

## Table of Contents

[TOC]

## 0. Motivation

At work we wanted to extract information from calls done using a CTI. This way we can improve the service we offer by getting insights from the call and by providing suggestions to agents. We can do that using different OpenAI models.

## 1. Extracting information with OpenAI

At the time of writing `ChatGPT 4o` is not able to process multimodal information. That means that we cannot pass an audio together with text to extract insights. To do so, we need to do it with 2 steps:

1. Transcribing the call with `whisper`
2. Extracting insights with `ChatGPT`

We will do it using <FancyLink linkText="OpenAI python" url="https://github.com/openai/openai-python" dark="true"/> package.

### 1.1. Using `Whisper` to transcribe the call

With `whisper` we can transcribe an audio file easily.
It can be done with:

```python
import io
import boto3
from openai import OpenAI

s3 = boto3.client("s3")
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))


def get_file(s3_uri):
    # Extract bucket and filename
    bucket, filename = s3_uri.replace("s3://", "").split("/", 1)

    obj = s3.get_object(Bucket=bucket, Key=filename)
    file = io.BytesIO(obj["Body"].read())
    file.name = s3_uri # Needed so that it knows the extension (mp3 mostly)
    return file

def transcribe_audio(client, file)
    return client.audio.transcriptions.create(model="whisper-1", file=file)

file = get_file("s3//your_bucket/your/file/path.mp3")
transcription = transcribe_audio(client, file)
```
<Notice type="warning">
  You will need to adapt the `file_path` and make sure to have the `OPENAI_API_KEY`. Alternativelly you can retrive the secret from somewhere else.
</Notice>

This code assumes that the file is stored in `S3` but it can easily be adapted for local files or for other cloud providers.

### 1.2. Using `ChatGPT` to extract insights

To extract information you can do a regular call to `ChatGPT`.
The key here is to use a prompt that asks for a `json` with any content you want.
Also, you can provide a <FancyLink linkText="Pydantic" url="https://docs.pydantic.dev/"/> class that the output should follow.

<Notice type="info">
  There is a lot of prompt engineering that you can do here, but this is out of the scope of this post.
</Notice>

So you can extract the information with:

````python
from openai import OpenAI

client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

PROMPT = """
I want you to extract information about a transcribed call. I want the output to be *only* a `json` with the following information:

* sentiment: positive/negative
* summary: a short summary of the call
* is_answered: true if the person who was being called answered the call else false
* successul_sale: true if the agent was able to sell a course
* improvements: improvements ideas for the agent that was doing the call
* failure_reason: in case there was no sale, the reason why. If there was a sale, leave it as `None` or empty

That json must be parsable with the following `pydantic` model:

```python
from pydantic import BaseModel
from typing import List, Optional, Literal

class CallAnalysis(BaseModel):
    sentiment: Literal['positive', 'negative']
    summary: str
    is_answered: bool
    successful_sale: bool
    improvements: List[str]
    failure_reason: Optional[str] = None
```

Here is the complete transcription:

```plaintext
{transcription}
```"""

def extract_info(transcription)
    messages = [{
        "role": "user",
        "content": PROMPT.format(transcription=transcription),
    }]

    chat_completion = client.chat.completions.create(
        model="gpt-4o", messages=messages,
    )

    return chat_completion.choices[0].message.content

extract_info("your transcription goes here")
````
<Notice type="warning">
  Remember to pass the `transcription` from the previous step.
</Notice>

## 2. Handling invalid calls

When processing the first calls, I noticed that we had a lot of super short calls where no transcription could be extracted.
Trying to transcribe those calls is a waste of resources.

To solve that I used <FancyLink linkText="Mutagen" url="https://github.com/quodlibet/mutagen" dark="true"/> to extract the audio length with:

```python
from mutagen.mp3 import HeaderNotFoundError
from mutagen.mp3 import MP3

def get_duration(file):
    """Get duration of the audio file using mutagen"""
    try:
        audio = MP3(file)
        duration_seconds = audio.info.length
    except HeaderNotFoundError:
        logger = get_run_logger()
        logger.error(f"Unable to read the duration of the file: {file.name}")
        duration_seconds = -1

    file.seek(0)  # Reset file pointer after reading
    return duration_seconds
```

Once extracted, I could only transcribe if the audio length was greater than a minimum value with:

```python
import time
from loguru import logger

MIN_SECONDS = 5 # Adjust if needed

t0 = time.monotonic()
duration_seconds = get_duration(file)

if duration_seconds > MIN_SECONDS:
    transcription = transcribe_audio(client, file)
    trans_time = time.monotonic() - t0
    logger.info(f"Successfully transcribed {filename=} in {trans_time:.2f} seconds")
else:
    logger.info(
        f"Skipping transcription for {filename=}, "
        f"{duration_seconds=:.2f} is less than {MIN_SECONDS=}"
    )
```

## 3. Adding metadata

## 4. Processing the outputs

## 5. Concurrent Async calls

### 5.1. Doing async calls

### 5.2. Rate limiter
