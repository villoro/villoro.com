---
slug: solving-problem-small-files-data-lake
title: Solving the probem with small files in the Data Lake
meta_title: Solving the probem with small files in the Data Lake
description: xx
date: 2024-07-05
image: "/images/blog/0049-handling-small-files.jpg"
category: DE
tags: [DE, Performance, Spark, DBT]
draft: false
---

## 0. Motivation

## 1. What's the problem

In data lakes, managing the size of partitions is crucial for maintaining optimal performance and cost-efficiency.
Partitions that are too small, generally less than 1 GB, can significantly degrade performance and inflate costs.
This issue is well-documented in the <FancyLink linkText="Delta Lake Optimisation Guide" url="https://www.linkedin.com/pulse/delta-lake-optimisation-guide-deenar-toraskar/"/>, which highlights the adverse effects of small partitions on data processing and querying efficiency.

### 1.1. Performance

According to the article <FancyLink linkText="Fixing the Small File Problem in Data Lake with Spark" url="https://medium.com/@satadru1998/%EF%B8%8F-fixing-the-small-file-problem-in-data-lake-with-spark-2cc9fbb86796" dark="true"/>:

> Small files can cause a performance degradation of up to **4x**.

This is because small files increase the number of tasks required to process the data, leading to higher overhead in task management and scheduling. Each task incurs a startup cost, and when there are many small tasks, this overhead can accumulate, resulting in significant slowdowns.

### 1.2. Costs

Another significant issue with small file partitions is the impact on costs.
When I was working at Glovo, my colleagues observed that reducing the number of small files in heavily queried tables resulted in a **10x cost reduction**.

This significant saving was primarily due to the decreased number of `S3 GET requests`, which are a major cost factor when querying small files.

### 1.3. Default Behavior of Data Lake Writers

By default, many data lake writers tend to create a large number of small files.
For instance, Apache Spark will write multiple small files if not properly configured.
Similarly, with AWS Athena, it is challenging to control the file sizes, often resulting in tiny files being created.
If not set up correctly, these default behaviors can lead to a proliferation of small files, exacerbating performance and cost issues in the data lake.

## 2. Good file size

Determining the optimal file size in a data lake can be challenging, as it depends on various factors such as the data processing engine, query patterns, and specific workload characteristics.
However, most documentation and data processing engines recommend targeting a file size **around 1 GB**.

For example, the <FancyLink linkText="Delta Lake Small File Compaction with OPTIMIZE" url="https://delta.io/blog/2023-01-25-delta-lake-small-file-compaction-optimize/"/> article highlights that Delta Lake suggests aiming for partitions close to **1 GB in size**. This recommendation helps balance the performance benefits of larger files with the practical considerations of data processing and storage.

Similarly, the <FancyLink linkText="DuckDB | Parquet File Sizes" url="https://duckdb.org/docs/guides/performance/file_formats.html#parquet-file-sizes"/> guide suggests that an optimal file size for Parquet files typically ranges **between 100 MB and 10 GB**. This broad range takes into account different use cases and workload requirements, allowing for flexibility while still emphasizing the importance of avoiding too small files.

<Notice type="success">
  Around **1 GB per file** is the usual target for optimal file size in data lakes.
</Notice>

## 3. How to avoid it?

The key to avoiding small files in your data lake is to control the number of files written during data processing.
Ensuring that the files are of optimal size improves both performance and cost-efficiency.

### 3.2. Spark

In Spark, you can control the number of files written by using the `sdf.repartition(n_files)` function, which allows you to specify the number of partitions (and hence the number of output files).
Additionally, you can configure Spark to manage the size of files by setting the minimum and maximum number of rows per file through the configuration options `spark.sql.files.maxRecordsPerFile` and `spark.sql.files.minRecordsPerFile`.

<Notice type="warning">
  Using `repartition` can cause significant shuffling of data, which might lead to performance issues.
  The number of partitions also affects Spark's parallelization; too few partitions can lead to underutilized resources, while too many can cause excessive overhead.
</Notice>

### 3.3. Athena with DBT

<Notice type="warning">
  In AWS Athena, **there is no direct way to control the number of files** written during query execution.
</Notice>

However, you can use DBT to manage this.
By implementing a DBT post-hook, you can compact the files after they are written using the `OPTIMIZE` command, which merges smaller files into larger ones.
This will only work for <FancyLink linkText="Iceberg" url="https://iceberg.apache.org/" company="iceberg"/> tables.
For more details, refer to the official documentation: <FancyLink linkText="Optimizing Iceberg tables" url="https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg-data-optimization.html" company="aws"/>

Running `OPTIMIZE` will generate additional files. To avoid extra storage costs and maintain optimal performance, it is important to run `VACUUM` on the table.
The `VACUUM` command removes any files that are no longer needed after the optimization process, freeing up storage space and reducing costs.

Here is the macros you will need:

<TerminalOutput color="stone">
  macros/iceberg.sql
</TerminalOutput>
```sql
{%- macro optimize(table=this) -%}
    {%- if (model.config.table_type == "iceberg")
        and (model.config.materialized != "view")
        and (adapter.type() == "athena")
    -%}
        OPTIMIZE {{ table }} REWRITE DATA USING BIN_PACK;
    {%- endif -%}
{%- endmacro -%}

{%- macro vacuum(table=this) -%}
    {%- if (model.config.table_type == "iceberg")
        and (model.config.materialized != "view")
        and (adapter.type() == "athena")
    -%}
        VACUUM {{ table }};
    {%- endif -%}
{%- endmacro -%}
```

And the docs for those macros:

<TerminalOutput color="stone">
  macros/iceberg.yml
</TerminalOutput>
```yaml
version: 2

macros:
  - name: optimize
    description: |
      Optimze iceberg tables (and skip it on duckdb).
      More info: https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg-data-optimization.html#querying-iceberg-data-optimization-rewrite-data-action
    arguments:
      - name: table
        type: string
        description: Table to `optimize`, by default it will use `this`

  - name: vacuum
    description: |
      Vacuum iceberg tables (and skip it on duckdb).
      More info: https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg-data-optimization.html#querying-iceberg-vacuum
    arguments:
      - name: table
        type: string
        description: Table to `vacuum`, by default it will use `this`
```

And then, you can declare the **post-hook** in the `dbt_project.yml`:

<TerminalOutput color="stone">
  dbt_project.yml
</TerminalOutput>
```yaml
models:
  your_project:
    +post-hook:
      - "{{ optimize() }}"
      - "{{ vacuum() }}"
```

## 4. Good partitioning

When it comes to partitioning tables in a data lake, it's important to consider the type of data and its usage patterns.
There are generally two types of tables:

* **Fact tables:** These tables typically store raw data and are often partitioned by a `p_updated_at`, `p_extracted_at`, or a similar time-based column. Partitioning by time allows for efficient querying of recent data but can lead to many small files if the partitioning interval is too granular. For example, if data is extracted every 3 hours, it would result in approximately 1500 files per year.

* **Aggregated (or presentation) tables**: These tables represent a final snapshot or aggregated view of the data. They often don't need to be partitioned unless the dataset is very large. Partitioning might not be necessary for these tables as they are typically queried in their entirety or by specific dimensions that do not benefit significantly from partitioning.

### 4.1. Partitioning Fact tables

For fact tables, while partitioning by time is common, it can be more efficient to avoid partitioning if it leads to small files.
Using data formats like <FancyLink linkText="Delta Lake" url="https://delta.io/"/> or <FancyLink linkText="Apache Iceberg" url="https://iceberg.apache.org/" company="iceberg"/> can help manage this.

According to Databricks documentation (<FancyLink linkText="When to partition tables on Databricks" url="https://docs.databricks.com/en/tables/partitions.html"/>):

> Most Delta tables with less than 1 TB of data do not require partitions.

So in many cases, the best partition strategy is to not partition at all.
Given that we will be appending new data based on the `updated_at`/`extracted_at` column, we can use `row groups` (see: <FancyLink linkText="Parquet | Concepts" url="https://parquet.apache.org/docs/concepts/" company="parquet"/>) for effective data pruning even if there are no partitions.

With Iceberg, we can follow a similar strategy where small tables are not partitioned.
Both Delta Lake and Iceberg use `parquet` under the hood, enabling them to leverage the same techniques for smart data pruning.
However, Iceberg offers additional options.
Iceberg provides functions to partition the data effectively, such as `months(extracted_at)`, which can reduce the number of partitions and improve query performance.
See <FancyLink linkText="Iceberg | Partition Transforms" url="https://iceberg.apache.org/spec/#partition-transforms" company="iceberg"/>.

This is possible because of <FancyLink linkText="Iceberg's hidden partitioning" url="https://iceberg.apache.org/docs/1.4.0/partitioning/#icebergs-hidden-partitioning" company="iceberg"/>.

## 5. How to track the problem

There are ways for iceberg and delta to get the number of files and partitions each table has.
The idea is to regularly query the metadata for each table and export it into a table.
Ideally we export that data every time a table has been written.

### 5.1. Iceberg

With iceberg, we can query the `files` metadata which gives us something like this:

| content | file_path                          | partition          | record_count | file_size_in_bytes |
|---------|------------------------------------|--------------------|--------------|--------------------|
| 0       | s3:/.../table/data/00000-3.parquet | \{1999-01-01, 01\} | 1            | 597                |
| 0       | s3:/.../table/data/00001-4.parquet | \{1999-01-01, 02\} | 1            | 597                |
| 0       | s3:/.../table/data/00002-5.parquet | \{1999-01-01, 03\} | 1            | 597                |

<Notice type="info">
  Showing only some relevant columns
</Notice>

From there we can easily extract the info we want with something like:

```sql
SELECT
    '{{ this.schema }}.{{ this.table }}' AS table_name,
    count(*) AS total_files,
    sum(record_count) AS total_rows,
    round(sum(CAST(file_size_in_bytes AS DOUBLE)) / 1024 / 1024, 3) AS total_size_mb,
    CURRENT_TIMESTAMP AS exported_at
FROM {{ this.schema }}."{{ this.table }}$files";
```

#### 5.1.1. Iceberg in Spark

According to the offical documetnation (<FancyLink linkText="Spark: Querying with SQL | Iceberg" url="https://iceberg.apache.org/docs/latest/spark-queries/#querying-with-sql" company="iceberg"/>) we can query the `files` metadata with:

```sql
spark.sql(f"SELCT * FROM iceberg.{schema}.{table}.files")
```

<Notice type="warning">
  You must set up the `iceberg` catalog for that to work. Read <FancyLink linkText="Spark Configuration | Iceberg" url="https://iceberg.apache.org/docs/latest/spark-configuration/" company="iceberg"/> for more info.
</Notice>

With that in mind, we can create the function `export_table_stats` that must be called every time we write a table:

```python
import backoff
from prefect import get_run_logger

QUERY_STATS = """
SELECT
    '{schema}.{table}' AS table_name,
    count(*) AS total_files,
    sum(record_count) AS total_rows,
    round(sum(CAST(file_size_in_bytes AS DOUBLE)) / 1024 / 1024, 3) AS total_size_mb,
    CURRENT_TIMESTAMP AS exported_at
FROM iceberg.{schema}.{table}.files;
"""
TABLE_STATS = "iceberg.nt_bronze__metadata.raw_files"
ICEBERG_COMMIT_EXC = "org.apache.iceberg.exceptions.CommitFailedException"

class CommitFailedException(Exception):
    """Represents 'org.apache.iceberg.exceptions.CommitFailedException' exception"""

    pass

def log_error(details):
    logger = get_run_logger()

    msg = "'{target}' failed, backing off {wait:0.1f} seconds after {tries} tries"
    logger.warning(msg.format(**details))


@backoff.on_exception(
    backoff.expo, CommitFailedException, on_backoff=log_error, max_time=60 * 2  # 2 mins
)
def export_table_stats(spark, tablename):
    _, schema, table = tablename.split(".")

    query = QUERY_STATS.format(schema=schema, table=table)
    sdf = spark.sql(query)

    try:
        sdf.repartition(1).write.mode("append").format("parquet").saveAsTable(
            TABLE_STATS
        )

    # Catch the exception so that we can further inspect the type
    except Exception as exc:
        # If it's what we are looking for, raise our custom exception so that it can 'backoff'
        if ICEBERG_COMMIT_EXC == exc.java_exception.getClass().getName():
            raise CommitFailedException("Write failed (Commit Failed)")

        # If it's not the wanted exception, raise it
        raise exc
```
<Notice type="success">
  Then you only need to call `export_table_stats(spark, tablename)` after every write
</Notice>

In general Iceberg can handle concurrent write operations.
Read <FancyLink linkText="Concurrent write operations | Iceberg" url="https://iceberg.apache.org/docs/1.4.1/reliability/#concurrent-write-operations" company="iceberg"/> for more info.
But sometimes I had problems when exporting the stats.
That is why I added retries with <FancyLink linkText="Backoff" url="https://github.com/litl/backoff" dark="true"/>.

#### 5.1.2. Iceberg in Athena with DBT

You can query the `files` metadata with:

```sql
SELECT * FROM dbname."tablename$files"
```

For more info read <FancyLink linkText="Querying Iceberg table metadata | Athena" url="https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg-table-metadata.html" company="aws"/> (and/or the <FancyLink linkText="Iceberg $files table | Trino" url="https://trino.io/docs/current/connector/iceberg.html#files-table"/> docs which is the underlying querying engine for Athena)

In order to automate that, we can create the following macros:

<TerminalOutput color="stone">
  macros/metadata.sql
</TerminalOutput>
```sql
{%- macro create_metadata_schema() -%}
    {%- if is_scheduled_pro_run() -%}
        {% do adapter.create_schema(api.Relation.create(database=target.database, schema=var("metadata")["db"])) %}
    {%- endif -%}
{%- endmacro -%}

{%- macro create_metadata_table_raw_files() -%}
    {%- if is_scheduled_pro_run() -%}
        CREATE TABLE IF NOT EXISTS {{ var("metadata")["db"] }}.{{ var("metadata")["tables"]["raw_files"] }} (
            table_name STRING,
            total_files BIGINT,
            total_rows BIGINT,
            total_size_mb DOUBLE,
            exported_at TIMESTAMP
        )
        LOCATION '{{ get_bronze_bucket() }}/metadata/{{ var("metadata")["tables"]["raw_files"] }}'
        TBLPROPERTIES (
            'table_type' ='ICEBERG',
            'format' = 'parquet',
            'write_compression' = 'snappy',
            'vacuum_max_snapshot_age_seconds' = '86400'
        );
    {%- endif -%}
{%- endmacro -%}

{% macro get_raw_files_table() %}
    {{ var("metadata")["db"] }}.{{ var("metadata")["tables"]["raw_files"] }}
{% endmacro %}

{%- macro export_metadata() -%}
    {%- if is_scheduled_pro_run() and (model.config.table_type == "iceberg") and (model.config.materialized != "view") -%}
        INSERT INTO {{ get_raw_files_table() }}
        SELECT
            '{{ this.schema }}.{{ this.table }}' AS table_name,
            count(*) AS total_files,
            sum(record_count) AS total_rows,
            round(sum(CAST(file_size_in_bytes AS DOUBLE)) / 1024 / 1024, 3) AS total_size_mb,
            CURRENT_TIMESTAMP AS exported_at
        FROM {{ this.schema }}."{{ this.table }}$files";
    {%- endif -%}
{%- endmacro -%}

{%- macro optimize_metadata_table() -%}
    {%- if is_scheduled_pro_run() -%}
        {{ optimize(get_raw_files_table()) }}
    {%- endif -%}
{%- endmacro -%}

{%- macro vacuum_metadata_table() -%}
    {%- if is_scheduled_pro_run() and var('vacuum_raw_files') -%}
        {{ vacuum(get_raw_files_table()) }}
    {%- endif -%}
{%- endmacro -%}
```

<TerminalOutput color="stone">
  macros/metadata.yml
</TerminalOutput>
```yaml
version: 2

macros:
  - name: create_metadata_schema
    description: |
      Creates the schema 'metadata' when running in `Athena` and in **PRO** using `nt_pro` profile

  - name: create_metadata_table_raw_files
    description: |
      Creates the table 'raw_files' inside schema 'metadata'.
      This contains `iceberg` metadata about the files each table has.
      Will only be created when running in `Athena` and in **PRO** using `nt_pro` profile

  - name: export_metadata
    description: |
      Exports metadata about a specific table
      Will only be done when running in `Athena` and in **PRO** using `nt_pro` profile

  - name: get_raw_files_table
    description: Retrieves the fully qualified name of the raw files table from the metadata variables.

  - name: optimize_metadata_table
    description: Calls the `optimize` macro only when running in `Athena` and in **PRO** using `nt_pro` profile

  - name: vacuum_metadata_table
    description: |
      Calls the `vacuum` macro only when:

      * Running in `Athena`
      * Running in **PRO** using `nt_pro` profile
      * The variable `vacuum_raw_files` is `True` (happens when `--select` not present)
```

And then, you can declare the **post-hook** in the `dbt_project.yml`:

<TerminalOutput color="stone">
  dbt_project.yml
</TerminalOutput>
```yaml
models:
  your_project:
    +post-hook:
      - "{{ optimize() }}"
      - "{{ vacuum() }}"
      - "{{ export_metadata() }}" # <-- Add this
```

#### 5.1.3. Track `files_by_day`

Now that we have a table with a snapshot of the metadata of each table whenever there was a write, we can create a table that tracks the latest state by day.
This table will help us monitor partitioning problems.

Here is the DBT model for that:

<TerminalOutput color="stone">
  models/files_by_day.sql
</TerminalOutput>
```sql
{{ config(unique_key='_uuid') }}

WITH source AS (
    SELECT *
    FROM {{ source('nt_bronze__metadata', 'raw_files') }}
),

fillna AS (
    SELECT
        table_name,
        total_files,
        exported_at,
        COALESCE(total_rows, 0) AS total_rows,
        COALESCE(total_size_mb, 0) AS total_size_mb
    FROM source
),

daily AS (
    SELECT
        table_name,
        CAST(exported_at AS date) AS export_date,
        {{ last('total_files') }}
            OVER (PARTITION BY table_name ORDER BY exported_at ASC) AS total_files,
        {{ last('total_rows') }}
            OVER (PARTITION BY table_name ORDER BY exported_at ASC) AS total_rows,
        {{ last('total_size_mb') }}
            OVER (PARTITION BY table_name ORDER BY exported_at ASC) AS total_size_mb,
        RANK()
            OVER (PARTITION BY table_name, CAST(exported_at AS date) ORDER BY exported_at ASC)
            AS _rank
    FROM fillna
),

daily_with_lags AS (
    SELECT
        table_name,
        export_date,
        total_files,
        total_rows,
        total_size_mb,
        LAG(total_files)
            OVER (PARTITION BY table_name ORDER BY export_date ASC) AS last_total_files,
        LAG(total_rows)
            OVER (PARTITION BY table_name ORDER BY export_date ASC) AS last_total_rows,
        LAG(total_size_mb)
            OVER (PARTITION BY table_name ORDER BY export_date ASC) AS last_total_size_mb
    FROM daily
    WHERE _rank = 1
),

daily_with_additions AS (
    SELECT
        ----------  ids
        {{ dbt_utils.generate_surrogate_key(['table_name', 'export_date']) }} AS _uuid,
        table_name,
        SUBSTRING(table_name FROM 1 FOR POSITION('.' IN table_name) - 1) AS schema_name,
        export_date,
        ----------  totals
        total_files,
        total_rows,
        total_size_mb,
        ----------  added datas
        COALESCE(total_files - last_total_files, total_files) AS added_files,
        COALESCE(total_rows - last_total_rows, total_rows) AS added_rows,
        ROUND(COALESCE(total_size_mb - last_total_size_mb, total_size_mb), 3) AS added_mb
    FROM daily_with_lags
)

SELECT *
FROM daily_with_additions
```

<TerminalOutput color="stone">
  models/files_by_day.yml
</TerminalOutput>
```yaml
models:
  - name: files_by_day
    description: Shows total and added `files`, `rows` and `MBs` of each table
    columns:
      # ----------  ids
      - name: _uuid
        description: Unique identifier of the table. Composite key of `table_name` and `export_date`
        tests:
          - not_null
          - unique
      - name: table_name
        description: Full name of the table (format `schema.table`)
        tests: [not_null]
      - name: schema_name
        description: Name of the schema, extracted from `table_name`
      - name: export_date
        description: Day the statistics where exported for the given table
        tests: [not_null]

      # ----------  totals
      - name: total_files
        description: Total number of files the table has
        tests: [not_null]
      - name: total_rows
        description: Total number of rows the table has
        tests: [not_null]
      - name: total_size_mb
        description: Total size of the table in MBs
        tests: [not_null]

      # ----------  added
      - name: added_files
        description: Added number of files from the last write
        tests: [not_null]
      - name: added_rows
        description: Added rows from the last write
        tests: [not_null]
      - name: added_mb
        description: Increase in size (as `MB`) from the last write
        tests: [not_null]
```

### 5.2. Delta

With delta we can use the `DESCRIBE DETAIL table` command to get similar information. 
That produces a table like this:

| name               | location             | createdAt           | lastModified        | partitionColumns | numFiles | sizeInBytes |
|--------------------|----------------------|---------------------|---------------------|------------------|---------:|------------:|
| default.deltatable | file:/Users/tuor/... | 2020-06-05 12:20:20 | 2020-06-05 12:20:20 |               [] |       10 |       12345 |

<Notice type="info">
  Showing only some relevant columns
</Notice>

You can read more about this in the docs <FancyLink linkText="Review Delta Lake table details with describe detail" url="https://docs.databricks.com/en/delta/table-details.html#review-delta-lake-table-details-with-describe-detail"/>

Regarding when and how to extract that metadata, you can follow a similar approach to what we saw for Iceberg.

## 6. Fixing the problem

After following the previous steps you have detected multiple tables that are not well partitioned since they have a lot of small files.
What can you do? How can you fix the data?

### 6.1. Non partitioned tables

For non partitioned tables is quite straightforward. You can simply run `OPTIMIZE` to compact the table.

<Notice type="warning">
  The `optimize` can take a lot of time and might require a lot of processing power (depends on the table size). Proceed with caution.
</Notice>

### 6.2. Partitioned tables

The best case scenario for partitioned tables is where you can compact the partitions with `OPTIMIZE`.
That is the same as for the non partitioned tables.

If you want, you can optimze only some partitions, which will be faster and require less computing power.

If compacting the partitions is not enough, you will need to change the partitioning of the table.
That works differently for Delta and Iceberg tables.

### 6.2.1. Partitioned Delta tables

Delta tables have fixed partitioning and you **cannot change the partitioning** of an existing table.
So you will need to recreate the table without the partition colum or with a different partitioning.

You can do that by first creating a new table with:

1. Create a new temporal table with the new partitioning you want
2. Move all the table from the original table to the temporal one
3. Optimize the new table
4. Validate that the new table

<Notice type="info">
  We optimize the temporal table so that all changes are done before removing the orignal table.
  This way we minimize downtime.
</Notice>

To create the table you can simply do a `CREATE TABLE AS` like:

```sql
CREATE TABLE {db}.{new}
  WITH (
    table_type = 'iceberg',
    is_external = false,
    location ='s3://{bucket}/{db_path}/{new}/'
  ) AS
  SELECT * 
  FROM {db}.{old}
  --Optionally add a WHERE clause to create it with a subset of data
```

<Notice type="warning">
  You might have problems creating a table with all the data with one query.
  In that case create the table with only some partitions and the do `INSERTS` to move all data.
</Notice>

Once everything is good on the new table, you can recreate the original table with:

1. Delete the original table
2. Recreate the original table
3. Move back all the data
4. Delete the temporal table

### 6.2.2. Partitioned Iceberg tables

With Iceberg you can do schema evolutions, such as changing the partitioning.
You can read about it at <FancyLink linkText="Iceberg evolutions" url="https://iceberg.apache.org/docs/nightly/evolution/"/>.

However there is a problem:

<Notice type="error">
  Changing the partitioning of a delta table **does not repartition old data**.
</Notice>

So if you need to compact old partitions you still will need to move the data to a temporal table and then bring it back.
You can follow a similar approach to what we saw for Delta tables.
