---
slug: solving-problem-small-files-data-lake
title: Solving the probem with small files in the Data Lake
meta_title: Solving the probem with small files in the Data Lake
description: xx
date: 2024-07-05
image: "/images/blog/0049-handling-small-files.jpg"
category: DE
tags: [DE, Performance, Spark]
draft: false
---

## 0. Motivation

## 1. What's the problem

In data lakes, managing the size of partitions is crucial for maintaining optimal performance and cost-efficiency.
Partitions that are too small, generally less than 1 GB, can significantly degrade performance and inflate costs.
This issue is well-documented in the <FancyLink linkText="Delta Lake Optimisation Guide" url="https://www.linkedin.com/pulse/delta-lake-optimisation-guide-deenar-toraskar/"/>, which highlights the adverse effects of small partitions on data processing and querying efficiency.

### 1.1. Performance

According to the article <FancyLink linkText="Fixing the Small File Problem in Data Lake with Spark" url="https://medium.com/@satadru1998/%EF%B8%8F-fixing-the-small-file-problem-in-data-lake-with-spark-2cc9fbb86796" dark="true"/>:

> Small files can cause a performance degradation of up to **4x**.

This is because small files increase the number of tasks required to process the data, leading to higher overhead in task management and scheduling. Each task incurs a startup cost, and when there are many small tasks, this overhead can accumulate, resulting in significant slowdowns.

### 1.2. Costs

Another significant issue with small file partitions is the impact on costs.
When I was working at Glovo, my colleagues observed that reducing the number of small files in heavily queried tables resulted in a **10x cost reduction**.

This significant saving was primarily due to the decreased number of `S3 GET requests`, which are a major cost factor when querying small files.

### 1.3. Default Behavior of Data Lake Writers

By default, many data lake writers tend to create a large number of small files.
For instance, Apache Spark will write multiple small files if not properly configured.
Similarly, with AWS Athena, it is challenging to control the file sizes, often resulting in tiny files being created.
If not set up correctly, these default behaviors can lead to a proliferation of small files, exacerbating performance and cost issues in the data lake.

## 2. Good file size

Determining the optimal file size in a data lake can be challenging, as it depends on various factors such as the data processing engine, query patterns, and specific workload characteristics.
However, most documentation and data processing engines recommend targeting a file size **around 1 GB**.

For example, the <FancyLink linkText="Delta Lake Small File Compaction with OPTIMIZE" url="https://delta.io/blog/2023-01-25-delta-lake-small-file-compaction-optimize/"/> article highlights that Delta Lake suggests aiming for partitions close to **1 GB in size**. This recommendation helps balance the performance benefits of larger files with the practical considerations of data processing and storage.

Similarly, the <FancyLink linkText="DuckDB | Parquet File Sizes" url="https://duckdb.org/docs/guides/performance/file_formats.html#parquet-file-sizes"/> guide suggests that an optimal file size for Parquet files typically ranges **between 100 MB and 10 GB**. This broad range takes into account different use cases and workload requirements, allowing for flexibility while still emphasizing the importance of avoiding too small files.

<Notice type="success">
  Around **1 GB per file** is the usual target for optimal file size in data lakes.
</Notice>

## 3. How to avoid it?

The key to avoiding small files in your data lake is to control the number of files written during data processing.
Ensuring that the files are of optimal size improves both performance and cost-efficiency.

### 3.2. Spark

In Spark, you can control the number of files written by using the `sdf.repartition(n_files)` function, which allows you to specify the number of partitions (and hence the number of output files).
Additionally, you can configure Spark to manage the size of files by setting the minimum and maximum number of rows per file through the configuration options `spark.sql.files.maxRecordsPerFile` and `spark.sql.files.minRecordsPerFile`.

<Notice type="warning">
  Using `repartition` can cause significant shuffling of data, which might lead to performance issues.
  The number of partitions also affects Spark's parallelization; too few partitions can lead to underutilized resources, while too many can cause excessive overhead.
</Notice>

### 3.3. Athena with DBT

<Notice type="warning">
  In AWS Athena, **there is no direct way to control the number of files** written during query execution.
</Notice>

However, you can use DBT to manage this.
By implementing a DBT post-hook, you can compact the files after they are written using the `OPTIMIZE` command, which merges smaller files into larger ones.
This will only work for <FancyLink linkText="Iceberg" url="https://iceberg.apache.org/" company="iceberg"/> tables.
For more details, refer to the official documentation: <FancyLink linkText="Optimizing Iceberg tables" url="https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg-data-optimization.html" company="aws"/>

Running `OPTIMIZE` will generate additional files. To avoid extra storage costs and maintain optimal performance, it is important to run `VACUUM` on the table.
The `VACUUM` command removes any files that are no longer needed after the optimization process, freeing up storage space and reducing costs.

Here is the macros you will need:

<TerminalOutput color="stone">
  macros/iceberg.sql
</TerminalOutput>
```sql
{%- macro optimize(table=this) -%}
    {%- if (model.config.table_type == "iceberg")
        and (model.config.materialized != "view")
        and (adapter.type() == "athena")
    -%}
        OPTIMIZE {{ table }} REWRITE DATA USING BIN_PACK;
    {%- endif -%}
{%- endmacro -%}

{%- macro vacuum(table=this) -%}
    {%- if (model.config.table_type == "iceberg")
        and (model.config.materialized != "view")
        and (adapter.type() == "athena")
    -%}
        VACUUM {{ table }};
    {%- endif -%}
{%- endmacro -%}
```

And the docs for those macros:

<TerminalOutput color="stone">
  macros/iceberg.yml
</TerminalOutput>
```yaml
version: 2

macros:
  - name: optimize
    description: |
      Optimze iceberg tables (and skip it on duckdb).
      More info: https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg-data-optimization.html#querying-iceberg-data-optimization-rewrite-data-action
    arguments:
      - name: table
        type: string
        description: Table to `optimize`, by default it will use `this`

  - name: vacuum
    description: |
      Vacuum iceberg tables (and skip it on duckdb).
      More info: https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg-data-optimization.html#querying-iceberg-vacuum
    arguments:
      - name: table
        type: string
        description: Table to `vacuum`, by default it will use `this`
```

And then, you can declare the **post-hook** in the `dbt_project.yml`:

<TerminalOutput color="stone">
  dbt_project.yml
</TerminalOutput>
```yaml
models:
  your_project:
    +post-hook:
      - "{{ optimize() }}"
      - "{{ vacuum() }}"
```

## 4. Good partitioning

When it comes to partitioning tables in a data lake, it's important to consider the type of data and its usage patterns.
There are generally two types of tables:

* **Fact tables:** These tables typically store raw data and are often partitioned by a `p_updated_at`, `p_extracted_at`, or a similar time-based column. Partitioning by time allows for efficient querying of recent data but can lead to many small files if the partitioning interval is too granular. For example, if data is extracted every 3 hours, it would result in approximately 1500 files per year.

* **Aggregated (or presentation) tables**: These tables represent a final snapshot or aggregated view of the data. They often don't need to be partitioned unless the dataset is very large. Partitioning might not be necessary for these tables as they are typically queried in their entirety or by specific dimensions that do not benefit significantly from partitioning.

### 4.1. Partitioning Fact tables

For fact tables, while partitioning by time is common, it can be more efficient to avoid partitioning if it leads to small files.
Using data formats like <FancyLink linkText="Delta Lake" url="https://delta.io/"/> or <FancyLink linkText="Apache Iceberg" url="https://iceberg.apache.org/" company="iceberg"/> can help manage this.

According to Databricks documentation (<FancyLink linkText="When to partition tables on Databricks" url="https://docs.databricks.com/en/tables/partitions.html"/>):

> Most Delta tables with less than 1 TB of data do not require partitions.

So in many cases, the best partition strategy is to not partition at all.
Given that we will be appending new data based on the `updated_at`/`extracted_at` column, we can use `row groups` (see: <FancyLink linkText="Parquet | Concepts" url="https://parquet.apache.org/docs/concepts/" company="parquet"/>) for effective data pruning even if there are no partitions.

With Iceberg, we can follow a similar strategy where small tables are not partitioned.
Both Delta Lake and Iceberg use `parquet` under the hood, enabling them to leverage the same techniques for smart data pruning.
However, Iceberg offers additional options.
Iceberg provides functions to partition the data effectively, such as `months(extracted_at)`, which can reduce the number of partitions and improve query performance.
See <FancyLink linkText="Iceberg | Partition Transforms" url="https://iceberg.apache.org/spec/#partition-transforms" company="iceberg"/>.

This is possible because of <FancyLink linkText="Iceberg's hidden partitioning" url="https://iceberg.apache.org/docs/1.4.0/partitioning/#icebergs-hidden-partitioning" company="iceberg"/>.

## 5. How to track the problem

### 5.1. Spark

### 5.2. Athena

## 6. Fixing the problem

### 6.1. Iceberg

### 6.2. Delta
