---
slug: solving-problem-small-files-data-lake
title: Solving the probem with small files in the Data Lake
meta_title: Solving the probem with small files in the Data Lake
description: xx
date: 2024-07-05
image: "/images/blog/0049-handling-small-files.jpg"
category: DE
tags: ["DE", "Performance", "Spark"]
draft: false
---

## 0. Motivation

## 1. What's the problem

In data lakes, managing the size of partitions is crucial for maintaining optimal performance and cost-efficiency.
Partitions that are too small, generally less than 1 GB, can significantly degrade performance and inflate costs.
This issue is well-documented in the <FancyLink linkText="Delta Lake Optimisation Guide" url="https://www.linkedin.com/pulse/delta-lake-optimisation-guide-deenar-toraskar/"/>, which highlights the adverse effects of small partitions on data processing and querying efficiency.

### 1.1. Performance

According to the article <FancyLink linkText="Fixing the Small File Problem in Data Lake with Spark" url="https://medium.com/@satadru1998/%EF%B8%8F-fixing-the-small-file-problem-in-data-lake-with-spark-2cc9fbb86796" dark="true"/>:

> Small files can cause a performance degradation of up to **4x**.

This is because small files increase the number of tasks required to process the data, leading to higher overhead in task management and scheduling. Each task incurs a startup cost, and when there are many small tasks, this overhead can accumulate, resulting in significant slowdowns.

### 1.2. Costs

Another significant issue with small file partitions is the impact on costs.
When I was working at Glovo, my colleagues observed that reducing the number of small files in heavily queried tables resulted in a **10x cost reduction**.

This significant saving was primarily due to the decreased number of `S3 GET requests`, which are a major cost factor when querying small files.

## 2. Good file size

1 GB: <FancyLink linkText="Delta Lake Small File Compaction with OPTIMIZE" url="https://delta.io/blog/2023-01-25-delta-lake-small-file-compaction-optimize/"/>
100 MB to 10 GB: <FancyLink linkText="DuckDB | Parquet File Sizes" url="https://duckdb.org/docs/guides/performance/file_formats.html#parquet-file-sizes"/>

## 3. How to avoid it?

### 3.1. General approach

### 3.2. Spark

### 3.3. Athena with DBT

## 4. Good partitioning

## 5. How to track the problem

## 6. Fixing the problem

### 6.1. Iceberg

### 6.2. Delta
