---
slug: solving-problem-small-files-data-lake
title: Solving the probem with small files in the Data Lake
meta_title: Solving the probem with small files in the Data Lake
description: xx
date: 2024-07-05
image: "/images/blog/0049-handling-small-files.jpg"
category: DE
tags: [DE, Performance, Spark]
draft: false
---

## 0. Motivation

## 1. What's the problem

In data lakes, managing the size of partitions is crucial for maintaining optimal performance and cost-efficiency.
Partitions that are too small, generally less than 1 GB, can significantly degrade performance and inflate costs.
This issue is well-documented in the <FancyLink linkText="Delta Lake Optimisation Guide" url="https://www.linkedin.com/pulse/delta-lake-optimisation-guide-deenar-toraskar/"/>, which highlights the adverse effects of small partitions on data processing and querying efficiency.

### 1.1. Performance

According to the article <FancyLink linkText="Fixing the Small File Problem in Data Lake with Spark" url="https://medium.com/@satadru1998/%EF%B8%8F-fixing-the-small-file-problem-in-data-lake-with-spark-2cc9fbb86796" dark="true"/>:

> Small files can cause a performance degradation of up to **4x**.

This is because small files increase the number of tasks required to process the data, leading to higher overhead in task management and scheduling. Each task incurs a startup cost, and when there are many small tasks, this overhead can accumulate, resulting in significant slowdowns.

### 1.2. Costs

Another significant issue with small file partitions is the impact on costs.
When I was working at Glovo, my colleagues observed that reducing the number of small files in heavily queried tables resulted in a **10x cost reduction**.

This significant saving was primarily due to the decreased number of `S3 GET requests`, which are a major cost factor when querying small files.

### 1.3. Default Behavior of Data Lake Writers

By default, many data lake writers tend to create a large number of small files.
For instance, Apache Spark will write multiple small files if not properly configured.
Similarly, with AWS Athena, it is challenging to control the file sizes, often resulting in tiny files being created.
If not set up correctly, these default behaviors can lead to a proliferation of small files, exacerbating performance and cost issues in the data lake.

## 2. Good file size

Determining the optimal file size in a data lake can be challenging, as it depends on various factors such as the data processing engine, query patterns, and specific workload characteristics.
However, most documentation and data processing engines recommend targeting a file size **around 1 GB**.

For example, the <FancyLink linkText="Delta Lake Small File Compaction with OPTIMIZE" url="https://delta.io/blog/2023-01-25-delta-lake-small-file-compaction-optimize/"/> article highlights that Delta Lake suggests aiming for partitions close to **1 GB in size**. This recommendation helps balance the performance benefits of larger files with the practical considerations of data processing and storage.

Similarly, the <FancyLink linkText="DuckDB | Parquet File Sizes" url="https://duckdb.org/docs/guides/performance/file_formats.html#parquet-file-sizes"/> guide suggests that an optimal file size for Parquet files typically ranges **between 100 MB and 10 GB**. This broad range takes into account different use cases and workload requirements, allowing for flexibility while still emphasizing the importance of avoiding too small files.

<Notice type="success">
  Around **1 GB per file** is the usual target for optimal file size in data lakes.
</Notice>

## 3. How to avoid it?

The key to avoiding small files in your data lake is to control the number of files written during data processing.
Ensuring that the files are of optimal size improves both performance and cost-efficiency.

### 3.2. Spark

In Spark, you can control the number of files written by using the `sdf.repartition(n_files)` function, which allows you to specify the number of partitions (and hence the number of output files).
Additionally, you can configure Spark to manage the size of files by setting the minimum and maximum number of rows per file through the configuration options `spark.sql.files.maxRecordsPerFile` and `spark.sql.files.minRecordsPerFile`.

### 3.3. Athena with DBT

<Notice type="warning">
  In AWS Athena, **there is no direct way to control the number of files** written during query execution.
</Notice>

However, you can use DBT to manage this. With a DBT post-hook, you can compact the files after they are written using the `OPTIMIZE` command to merge smaller files into larger ones.
That will only work if your tables are <FancyLink linkText="Iceberg" url="https://iceberg.apache.org/" company="iceberg"/> tables.

Running `OPTIMIZE` will generate extra files.
In order to avoid extra storage costs you should `VACUUM` the table.

Here is the macros you will need:

<TerminalOutput color="stone">
  macros/iceberg.sql
</TerminalOutput>
```sql
{%- macro optimize(table=this) -%}
    {%- if (model.config.table_type == "iceberg")
        and (model.config.materialized != "view")
        and (adapter.type() == "athena")
    -%}
        OPTIMIZE {{ table }} REWRITE DATA USING BIN_PACK;
    {%- endif -%}
{%- endmacro -%}

{%- macro vacuum(table=this) -%}
    {%- if (model.config.table_type == "iceberg")
        and (model.config.materialized != "view")
        and (adapter.type() == "athena")
    -%}
        VACUUM {{ table }};
    {%- endif -%}
{%- endmacro -%}
```

And the docs for those macros:

<TerminalOutput color="stone">
  macros/iceberg.yml
</TerminalOutput>
```yaml
version: 2

macros:
  - name: optimize
    description: |
      Optimze iceberg tables (and skip it on duckdb).
      More info: https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg-data-optimization.html#querying-iceberg-data-optimization-rewrite-data-action
    arguments:
      - name: table
        type: string
        description: Table to `optimize`, by default it will use `this`

  - name: vacuum
    description: |
      Vacuum iceberg tables (and skip it on duckdb).
      More info: https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg-data-optimization.html#querying-iceberg-vacuum
    arguments:
      - name: table
        type: string
        description: Table to `vacuum`, by default it will use `this`
```

And then, you can declare the **post-hook** in the `dbt_project.yml`:

<TerminalOutput color="stone">
  dbt_project.yml
</TerminalOutput>
```yaml
models:
  your_project:
    +post-hook:
      - "{{ optimize() }}"
      - "{{ vacuum() }}"
```

## 4. Good partitioning

## 5. How to track the problem

## 6. Fixing the problem

### 6.1. Iceberg

### 6.2. Delta
