---
slug: solving-problem-small-files-data-lake
title: Solving the probem with small files in the Data Lake
meta_title: Solving the probem with small files in the Data Lake
description: xx
date: 2024-07-05
image: "/images/blog/0049-handling-small-files.jpg"
category: DE
tags: ["DE", "Performance", "Spark"]
draft: false
---

## 0. Motivation

## 1. What's the problem

In data lakes, managing the size of partitions is crucial for maintaining optimal performance and cost-efficiency.
Partitions that are too small, generally less than 1 GB, can significantly degrade performance and inflate costs.
This issue is well-documented in the <FancyLink linkText="Delta Lake Optimisation Guide" url="https://www.linkedin.com/pulse/delta-lake-optimisation-guide-deenar-toraskar/"/>, which highlights the adverse effects of small partitions on data processing and querying efficiency.

### 1.1. Performance

According to the article <FancyLink linkText="Fixing the Small File Problem in Data Lake with Spark" url="https://medium.com/@satadru1998/%EF%B8%8F-fixing-the-small-file-problem-in-data-lake-with-spark-2cc9fbb86796" dark="true"/>:

> Small files can cause a performance degradation of up to **4x**.

This is because small files increase the number of tasks required to process the data, leading to higher overhead in task management and scheduling. Each task incurs a startup cost, and when there are many small tasks, this overhead can accumulate, resulting in significant slowdowns.

### 1.2. Costs

Another significant issue with small file partitions is the impact on costs.
When I was working at Glovo, my colleagues observed that reducing the number of small files in heavily queried tables resulted in a **10x cost reduction**.

This significant saving was primarily due to the decreased number of `S3 GET requests`, which are a major cost factor when querying small files.

### 1.3. Default Behavior of Data Lake Writers

By default, many data lake writers tend to create a large number of small files.
For instance, Apache Spark will write multiple small files if not properly configured.
Similarly, with AWS Athena, it is challenging to control the file sizes, often resulting in tiny files being created.
If not set up correctly, these default behaviors can lead to a proliferation of small files, exacerbating performance and cost issues in the data lake.

## 2. Good file size

Determining the optimal file size in a data lake can be challenging, as it depends on various factors such as the data processing engine, query patterns, and specific workload characteristics.
However, most documentation and data processing engines recommend targeting a file size **around 1 GB**.

For example, the <FancyLink linkText="Delta Lake Small File Compaction with OPTIMIZE" url="https://delta.io/blog/2023-01-25-delta-lake-small-file-compaction-optimize/"/> article highlights that Delta Lake suggests aiming for partitions close to **1 GB in size**. This recommendation helps balance the performance benefits of larger files with the practical considerations of data processing and storage.

Similarly, the <FancyLink linkText="DuckDB | Parquet File Sizes" url="https://duckdb.org/docs/guides/performance/file_formats.html#parquet-file-sizes"/> guide suggests that an optimal file size for Parquet files typically ranges **between 100 MB and 10 GB**. This broad range takes into account different use cases and workload requirements, allowing for flexibility while still emphasizing the importance of avoiding too small files.

<Notice type="success">
  Around **1 GB per file** is the usual target for optimal file size in data lakes.
</Notice>

## 3. How to avoid it?

### 3.1. General approach

### 3.2. Spark

### 3.3. Athena with DBT

## 4. Good partitioning

## 5. How to track the problem

## 6. Fixing the problem

### 6.1. Iceberg

### 6.2. Delta
