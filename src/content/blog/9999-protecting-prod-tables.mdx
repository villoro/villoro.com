---
slug: protecting-prod-tables-dbt
title: Protecting production tables in DBT
meta_title: xxx
description: xx
date: 2025-09-04
image: /images/blog/9999-greek-protecting-soldiers.jpg
category: DE
tags: [XXX]
draft: false
---

## Ways of protecting production tables

1. Protecting against bad data
2. Protecting against unexpected changes
3. Portecting sensitvie data

## Protecting against bad data

The idea is simple, create an internal model (not exposed to end users) with the tests you need. And then a second model that depends on the first. This model will only be updated if the data is good (meaning that the tests on the internal model passed).

![Protecting against bad data](../../images/posts/2025/9999-protecting-bad-data.jpg)

This should only be used when it's more important to have high quality data than fresh data. Generally I recommend this approach only when there is something that uses this table and that if it goes wrong could be really problematic. Like imagine if we are sending emails to users based on a table, we want to make sure we only notify them when it's needed to avoid sending unwanted emails.

## Protecting against unexpected changes

What we want is to make sure we don't change the content of a table by mistake. This is useful when changes in a table trigger subsequent actions. For example, imagine we upload that data to some system and a lot of changes could mean a lot of expensive API calls. In that case, we want to protect that table.

The idea is to have a model that compares the differences between the developer version of a table and the production one.
This way the developer can check if those changes are inteded or not and go forward with the pull request based on that.

![Protecting against unexpected changes](../../images/posts/2025/9999-protect-unexpected-changes.jpg)

This is achieved using the `data_diff` macro:

<TerminalOutput color="stone">
  macros/data_diff.sql
</TerminalOutput>
```sql
{% macro data_diff(baseline, compare_with, pk, select=[], exclude=[]) %}
    {%- set cols = [] -%}
    {%- for c in adapter.get_columns_in_relation(baseline) -%}
        {%- if c.name == pk
            or (select and c.name in select)
            or (exclude and c.name not in exclude)
            or (not select and not exclude)
        -%}
            {%- do cols.append(c) -%}
        {%- endif -%}
    {%- endfor -%}
    {%- set cols_str = cols | map(attribute='quoted') | join(', ') -%}

    WITH a_except_b AS (
        SELECT {{ cols_str }} FROM {{ baseline }}
        {{ except() }}
        SELECT {{ cols_str }} FROM {{ compare_with }}
    ),
    b_except_a AS (
        SELECT {{ cols_str }} FROM {{ compare_with }}
        {{ except() }}
        SELECT {{ cols_str }} FROM {{ baseline }}

    ),
    different_pks AS (
        SELECT {{ pk }} FROM a_except_b
        UNION
        SELECT {{ pk }} FROM b_except_a
    ),
    different_a AS (
        SELECT '{{ baseline }}' AS relation, {{ cols_str }}
        FROM {{ baseline }}
        INNER JOIN different_pks USING ({{ pk }})
    ),
    different_b AS (
        SELECT '{{ compare_with }}' AS relation, {{ cols_str }}
        FROM {{ compare_with }}
        INNER JOIN different_pks USING ({{ pk }})
    ),
    unioned_differences AS (
        SELECT * FROM different_a
        UNION ALL
        SELECT * FROM different_b
    )

    SELECT * FROM unioned_differences
    ORDER BY {{ pk }}, relation
{% endmacro %}
```

That has this documentation:

<TerminalOutput color="stone">
  macros/data_diff.yml
</TerminalOutput>
```yaml
macros:
  - name: data_diff
    description: |
      Returns the rows that are different between two models, comparing only specified columns.
      The output table contains a `relation` column identifying the source model (`baseline` or `compare_with`),
      along with all compared columns from the models. Results are ordered by pk and relation.

    arguments:
      - name: baseline
        type: relation
        description: The baseline model to compare against

      - name: compare_with
        type: relation
        description: The model to compare with the baseline

      - name: pk
        type: string
        description: The primary key column used to identify rows

      - name: select
        type: list
        description: Optional list of columns to compare.

      - name: exclude
        type: list
        description: Optional list of columns to exclude from comparison.
```

Then we would create the comparison model. For example it could be:

<TerminalOutput color="stone">
  models/salesforce/data_diff__salesforce__users.sql
</TerminalOutput>
```sql
{# Enable it only on testing runs, you might need to change it #}
{{ config(
    enabled=target.name != 'pro' and target.type != 'duckdb'
) }}

{% set baseline = ref('salesforce__users') %}

{# Here we hardcore the production table #}
{% set compare_with = adapter.get_relation(
    database='awsdatacatalog',
    schema='pro__salesforce',
    identifier='salesforce__users'
) %}

WITH data_diff AS (
    {{ data_diff(
        baseline,
        compare_with,
        pk='user_id',
        select=[
            'name',
            'created_at',
            'last_updated_at',
            'p_extracted_at',
        ]
    ) }}
)

SELECT * FROM data_diff

```

## Protecting sentitive data

Imagine you have tables about the employers payrolls. It's really important that only a small subset of the users can read those raw tables.
The idea is that you will end up creating aggregates that are not that sensitve but you don't want people being able to access raw data.

To do that, you create a protected `database` that only `admins` (or any subset you want) can read it.
But then, when developers try to run DBT it fails because they cannot access those tables.

The idea for avoiding this problem is to have a `seed` for each protected table that mimics the real table but using fake data.

Then you can dynamically read from the table or the seed based on the role/profile.

![Protecting sentitive data](../../images/posts/2025/9999-protect-sensitive-data.jpg)

This would be done with something like:

```sql
WITH source AS (
    /*
        This allows us to use fake data when testing and real data in pro
        It's meant for sensitive data protection
    */
    SELECT *
    {% if is_scheduled_pro_run() -%}
        FROM {{ source('nt_raw__a3', 'absenteeisms') }}
    {% else -%}
        FROM {{ ref('fake_absenteeisms') }}
    {%- endif %}
)
-- downstream logic...
SELECT * FROM source;
```

<Notice type="info">
  Here we use `is_scheduled_pro_run` macro but the way you identify testing runs can be different.
</Notice>

It's really important to make sure the `seed` has the same schema as the real production table.
We have most data for DBT sources as string so that schema changes are easier to handle.
So we need to force the types on the seeds with something like:

```yaml
version: 2

seeds:
  - name: fake_absenteeisms
    description: Contains fake data for A3 absenteeisms
    meta:
      owner: Villoro

    columns:
      - name: absenteeism_id
        description: Absenteeisms pk.
      - name: type_of_absenteeism
        description: Category/type of IT absenteeism.
      - name: employee_id
        description: Reference to the employee related to the absenteeism.


    config:
      column_types:
        absenteeism_id: string
        type_of_absenteeism: string
        employee_id: string
```