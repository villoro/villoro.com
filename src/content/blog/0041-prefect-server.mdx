---
slug: prefect-server-setup
title: "Prefect Server Setup: Configuration and Deployment"
meta_title: Prefect Server Setup
description: xxxx
date: 2024-04-11
image: "/images/blog/0041-prefect-server.jpg"
tags: ["Tools", "Orchestration"]
draft: false
---

## Table of Contents

[TOC]

## 0. Intro

In this post I'll explain how to:

1. Set up your Prefect Server
2. Create `Deployments`
3. Connect to the Server

If you are not familiar with `Prefect` I suggest you read <FancyLink linkText="Prefect Essentials: Basics, Setup and Migration" url="https://villoro.com/blog/prefect-essentials-setup-and-migration/" dark="true"/> first.

## 1. Setting up a Prefect Server

<Notice type="info">
  This section is based on <FancyLink linkText="Hosting a Prefect server instance" url="https://docs.prefect.io/latest/guides/host/" dark="true"/> guide.
</Notice>

The simplest configuration that can work in `production` needs 3 pieces:

1. A **database** for the server
2. The `prefect server`
3. At least one `prefect worker`

For the `database` I simply used a `postgres` hosted on `AWS`.

Prefect is design to be able to scale way beyond that configuration (more info in <FancyLink linkText="Deploying Flows to Work Pools and Workers" url="https://docs.prefect.io/latest/guides/prefect-deploy/" dark="true"/>)
However in this post I'll only explain the basic setup.

This basic configuration works for me because I only want the `prefect workers` to trigger different `AWS` jobs and tasks using `boto3`, such as:
* `lambdas`
* `ecs` tasks with `fargate`
* `emr serverless` tasks

So one simple worker is enough for that set up.

### 1.1. Creating the Prefect Server

Here we only need a simple `docker` that has `prefect` installed.
There is no offical images for a `docker server` but we can use the official one for `workers`.

Then the only set up needed is to provide the correct configuration for the database:

<TerminalOutput color="stone">
  /Dockerfile.server
</TerminalOutput>
```docker
# Prefect version must match Dockerfile.worker and pyproject.toml
FROM prefecthq/prefect:2.16-python3.10 as base

# Set connection (format: `postgresql+asyncpg://{user}:{password}@{url}:{port}/prefect`)
ARG DB_URL
RUN prefect config set PREFECT_API_DATABASE_CONNECTION_URL="${DB_URL}"
RUN prefect config set PREFECT_SERVER_API_HOST=0.0.0.0

CMD ["prefect", "server", "start"]
```

<Notice type="warning">
  If you try to run this with the `database` in a `docker` you will probably run into networking problems.
  I strongly suggest you use <FancyLink linkText="Prefect example with Docker compose" url="https://github.com/rpeden/prefect-docker-compose/blob/main/docker-compose.yml" dark="true"/> to see how to do it with `docker compose`.
</Notice>

And then to create the `docker image` run:

```sh
docker build --build-arg DB_URL=XXX -f Dockerfile.server -t northius/prefect/server .
```
<Notice type="warning">
  Replace `XXX` with the database `URL`.
  If it's a `postgres` one it should be like `postgresql+asyncpg://{user}:{password}@{url}:{port}/prefect`
</Notice>

With the `prefect server` you will be able to access the web UI.
If you are running it locally you can find it in `localhost:4200`.
However the `server` won't be able to run any flow without a `worker`.

Once it works locally you will need to deploy it somewhere.
In my case I'm using `AWS ECS` and running it as a Service with `Fargate`.
Make sure to also provision a public URL so that the other pieces can connect to it.

<Notice type="warning">
  Make sure to set the server behind a **VPN** so that it's not accessible to everyone.
</Notice>

### 1.2. Creating a Prefect Worker

The first thing we need is to create a `Work Pool` so that the `Worker` we will create can join it.

<Notice type="info">
  We will use a `local subprocess` work pool.
  Given that we will only use it to trigger other services with `boto3` it should be more than enough.
  If needed, we can always change it to a `ECS` task and/or add more `workers`.
</Notice>

You can do it using Prefect UI:

![Create Prefect Work Pool](../../images/posts/2024/0041-prefect-work-pool.jpg)

Once you have the `Work Pool` we will create a `Worker` with docker, following a similar approach to the `Server`:

<TerminalOutput color="stone">
  /Dockerfile.worker
</TerminalOutput>
```docker
# Prefect version must match Dockerfile.server and pyproject.toml
FROM prefecthq/prefect:2.16-python3.10 as base

# Connect to prefect
ARG PREFECT_API_URL
ENV PREFECT_API_URL=${PREFECT_API_URL}

# Allow to config pool_name
ARG POOL_NAME=subprocess-pool
ENV POOL_NAME=${POOL_NAME}

ENTRYPOINT prefect worker start --pool ${POOL_NAME}
```
<Notice type="warning">
  Make sure that the `prefect` versions match in both `Dockerfile`.
</Notice>

Similarly to before, we can build the image with:

```sh
docker build --build-arg PREFECT_API_URL=XXX -f Dockerfile.worker -t northius/prefect/worker .
```
<Notice type="warning">
  Replace `XXX` with the prefect `API URL`.
  It should be like `https//{server_url}/api`.
</Notice>

Same as before, you will need to run this `docker` container somewhere.
In my case I'll also run it as another `ECS service` using `Fargate`.

## 2. Deployments

As the official documentations states in <FancyLink linkText="Prefect Deployments" url="https://docs.prefect.io/latest/concepts/deployments" dark="true"/>:

> **Prefect Deployments** are server-side representations of flows.
> They store the crucial metadata needed for remote orchestration including **when**, **where**, and **how** a workflow should run
> Deployments elevate workflows from functions that you must call manually to API-managed entities that can be triggered remotely.

In my case I'll use them to define:
* The different `flows` I want to run
* From where they will get the code (a github repo)
* When the `flow` should be run (`schedule`)

### 2.1. Getting the code

The first thing you will need to do is to create a `Prefect Block` (more info at <FancyLink linkText="Prefect Essentials: Basics, Setup and Migration | Using prefect blocks" url="https://villoro.com/blog/prefect-essentials-setup-and-migration/#3-using-prefect-blocks" dark="true"/>) to store the `Github` crendentials if the repo is not public.

Once done we can define **how** to get the code and the **dependencies** with:

<TerminalOutput color="stone">
  /prefect.yaml
</TerminalOutput>
```yaml
pull:
- prefect.deployments.steps.git_clone:
    id: clone-step
    repository: https://github.com/github_user/repo.git
    branch: main
    credentials: "{{ prefect.blocks.github.github }}"
- prefect.deployments.steps.pip_install_requirements:
    requirements_file: requirements.txt
    directory: "{{ clone-step.directory }}"
    stream_output: False
```
<Notice type="warning">
  This will only work if there is a `Github` block called `github` with a token that allows to read the repo.
  This is only needed if the repo is not public.
</Notice>

### 2.1. Creating the first Deployment

Now that we defined how to fetch the code we will create the first `Deployment`.
That is done by adding the following code to the `prefect.yaml` you just created.

<TerminalOutput color="stone">
  /prefect.yaml
</TerminalOutput>
```yaml
- name: lambdas.athena_history
  description: Export Athena history with a lambda
  entrypoint: src/lambdas/lambdas.py:athena_history
  version: 0.1.0
  work_pool:
    name: subprocess-pool
  tags: ["type:prefect", "group:lambda", "job:athena_history"]
```

Here is what we are setting:

| variable    | description                                                                     |
|-------------|---------------------------------------------------------------------------------|
| name        | The name you want for the deployment                                            |
| description | Description of what the deployment does                                         |
| entrypoint  | Where is located the flow. It is `path/to/filename.py:flow_name`                |
| version     | The version of the flow                                                         |
| work_pool   | Which work pool should run the deployment. It references what we created before |
| tags        | Any tags you want to add to all `flow_runs` that will come from this deployment |

Now you can deploy this to the `prefect server` with:

```sh
set PREFECT_API_URL=XXX
prefect deploy --all
```
<Notice type="warning">
  Replace `XXX` with the prefect `API URL`.
  It should be like `https//{server_url}/api`.
</Notice>

This will deploy all `Deployments` in `prefect.yaml` to the server.

Now you can head to the server and trigger an Run a `Deployment` from there.

### 2.3. Advanced Usage of Deployments

What we created works fine but once you start adding multiple Deployments you will probably have a lot of repetition.
To avoid it you can define some defaults that will be later used in each `Deployment` with:

<TerminalOutput color="stone">
  /prefect.yaml
</TerminalOutput>
```yaml
definitions:
  work_pools:
    subprocess-pool: &subprocess-pool
      name: subprocess-pool

    version: &version
      version: 0.1.0

    # Define defaults (that can be overwritten)
    defaults: &defaults
      <<: *version
      work_pool: *subprocess-pool
      tags: ["type:prefect"]
      schedule: {}
```

More info about <FancyLink linkText="YAML alias nodes" url="https://yaml.org/spec/1.2.2/#71-alias-nodes" dark="true"/>.

With that, you can reuse the defaults with:

<TerminalOutput color="stone">
  /prefect.yaml
</TerminalOutput>
```yaml
- name: lambdas.athena_history
  <<: *defaults
  description: Export Athena history with a lambda
  entrypoint: src/lambdas/lambdas.py:athena_history
  tags: ["type:prefect", "group:lambda", "job:athena_history"]

- name: lambdas.emr_history
  <<: *defaults
  description: Export EMR history with a lambda
  entrypoint: src/lambdas/lambdas.py:emr_history
  tags: ["type:prefect", "group:lambda", "job:emr_history"]
  schedule:
    cron: "0 3 * * *"
```
<Notice type="info">
  Notice that you can overwrite the `defaults`.
  For example here we are overwritting `tags`.
</Notice>

### 2.3. Deploying `deployments`

We saw how to manually deploy the `Deployments` but it's better to do it with CD.

#### 2.3.1. Deploys in CD

Here you will only need a `github action` (or any CD equivalent) that:

1. Detects changes to `prefect.yaml` on commits to the `main` branch
2. Deploys all `flows` to prefect

The only change you will need to do is to add `--no-prompt` so that it doesn't expect user confirmations:

```sh
prefect --no-prompt deploy --all
```

<Notice type="info">
  You can redeploy an already existing `Deployment` without losing any data.
  It will simply update whatever has changed.
</Notice>

#### 2.3.2. Manual deploy

One really cool thing you can do is to manually deploy `Deployments` when you are creating a new one.
Here what I do is to change the `branch` reference to the current branch where I am developing by

```diff
prefect.deployments.steps.git_clone:
    id: clone-step
    repository: https://github.com/github_user/repo.git
-    branch: main
+    branch: your_current_branch
    credentials: "{{ prefect.blocks.github.github }}"
```

And then you can deploy the `Deployment` you are working with:
```sh
prefect --no-prompt deploy --name lambdas.athena_history
```
<Notice type="warning">
  It is important to specify only the `Deployment` you are developing so that you don't interfere with the other `Deployments` you might have running in production.
</Notice>

Once you finish your work and do a commit to `main` the `CD` will overwrite the `branch` reference so that is points to `main`.
Notice that any `Deployment` run you do will `fetch` the code from the `branch` you set and run.
That implies that if you are changing only your code you do not need to `deploy` at each commit.
You will only need to `deploy` if you change:

* The reference `branch`
* The flow configuration (`name`, `parameters`, `tags` etc.)

## 3. Connect to prefect server

### 3.1. Connect EMR

### 3.2. Connect DBT

## 4. Prefect flows examples

<TerminalOutput color="stone">
  /src/common/prefect_utils.py
</TerminalOutput>
```python
import asyncio

from prefect import get_run_logger
from prefect.client import get_client
from prefect.context import get_run_context


def update_tags(tags):
    logger = get_run_logger()
    if not tags:
        logger.warning("No tags passed to 'update_tags', nothing to do")
        return True

    logger.info(f"Adding {tags=} to current flow_run")

    # Read current flow
    flow_run = get_run_context().flow_run
    tags += flow_run.tags

    client = get_client()
    asyncio.run(client.update_flow_run(flow_run.id, tags=set(tags)))
    return True

```

<TerminalOutput color="stone">
  /src/lambdas/base.py
</TerminalOutput>
```python
from prefect import get_run_logger
from prefect import task

from src.common.session import get_session


@task(name="prefect.lambdas.run")
def _run_lambda(client, name):
    logger = get_run_logger()

    response = client.invoke(
        FunctionName=name,
        InvocationType="RequestResponse",
        LogType="None",
    )

    if (status_code := response["StatusCode"]) == 200:
        logger.info(f"Lambda {name=} completed successfully")
        return True

    logger.error(f"Lambda {name=} failed with {status_code=}")


def run_lambda(env, name):
    logger = get_run_logger()
    logger.info(f"Running lambda {name=} in {env=}")

    client = get_session(env).client("lambda")
    _run_lambda(client, name)
```

<TerminalOutput color="stone">
  /src/lambdas/lambdas.py
</TerminalOutput>
```python
from typing import Literal

from prefect import flow

from prefect_northius.lambdas.base import run_lambda


@flow(name="prefect.lambdas.athena_history")
def athena_history(env: Literal["snd", "pro"]):
    run_lambda(env, "nt-lambda-function-athena-history")


@flow(name="prefect.lambdas.emr_history")
def emr_history(env: Literal["snd", "pro"]):
    run_lambda(env, "nt-lambda-function-emr-history")
```
