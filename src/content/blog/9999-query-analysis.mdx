---
slug: static-query-anlysis-sqlglot
title: Static Query Analysis with Sqlglot
meta_title: xx
description: xx
date: 2025-10-14
image: /images/blog/9999-query-tree.jpg
category: DE
tags: []
draft: false
---

## 1. Overview

This post explains how to extract and structure SQL query insights from Athena using **sqlglot** and a custom Python parser. The focus is on building a foundation for later performance analysis rather than interpreting the results yet.

The goal is to transform raw query history into a rich metadata dataset containing structural features, such as the number of joins, unions, CTEs, and flags for patterns like `SELECT *` or `ORDER BY` without `LIMIT`. These metrics help assess query complexity and prepare for identifying inefficient or costly query patterns across your organization.

This guide is especially relevant if you want to:

* Automatically parse large query logs (e.g., from Athena, BigQuery, or Snowflake).
* Build an internal SQL insights dataset for cost/performance optimization.
* Create a reproducible static analysis pipeline for SQL queries.

## 2. Problem

Data teams often accumulate thousands of historical SQL queries across BI tools, pipelines, and ad-hoc analysis. However, this data remains underutilized — stored as text without structure or insight.

Common challenges include:

* **Lack of visibility:** No systematic way to see which tables are used most, which queries are complex, or which teams produce heavy workloads.
* **Rising costs:** In systems like Athena, cost scales with data scanned. Inefficient queries (e.g., full-table scans, large joins, `SELECT *`) can silently drive up bills.
* **No feedback loop:** Engineers rarely get structured feedback about their query design.

The objective here is to **extract meaningful features from query text**. By parsing SQL with sqlglot, you can transform unstructured SQL into measurable metadata that supports:

* Identifying slow or expensive patterns.
* Designing best practices around query efficiency.
* Benchmarking how query design evolves over time.

The next sections detail the tools, parser, and pipeline that make this process scalable and maintainable.

<Notice type="warning">
  This approach focuses solely on **static analysis** — it inspects query text, not execution results. Runtime metrics like execution time, data scanned, or bytes processed should be retrieved directly from your query engine’s system tables or APIs to complement this analysis.
</Notice>

## 3. Extracting Query History (optional)

<Notice type="info">
  This section is optional and **Athena-specific**. The post focuses on static parsing, but if you need sample input data, here’s a minimal way to fetch Athena query history. Other engines have different APIs/system tables; adjust the listing & detail calls accordingly.
</Notice>

**Other engines at a glance**

* **BigQuery**: Jobs API (`jobs.list`, `jobs.get` → `statistics.query.totalBytesProcessed`, etc.)
* **Snowflake**: `QUERY_HISTORY` / `QUERY_HISTORY_BY_*` views
* **Databricks**: system tables (e.g., `system.query`) or REST APIs
* **Presto/Trino**: coordinator query REST API or server logs

Minimal, list-first approach using a **generator** for pagination and a compact batch fetch. This yields pages of IDs so you stay memory-light and can early-stop cleanly.

```python
import boto3

athena = boto3.client("athena")

def get_query_execution_ids(max_iterations=1000):
    """Yield pages (lists) of QueryExecutionIds."""
    resp = athena.list_query_executions()
    yield resp.get("QueryExecutionIds", [])

    next_token = resp.get("NextToken")
    for _ in range(max_iterations):
        if not next_token:
            break
        resp = athena.list_query_executions(NextToken=next_token)
        yield resp.get("QueryExecutionIds", [])
        next_token = resp.get("NextToken")


def get_query_executions(ids, batch_size=50):
    """Return a list[dict] with only the fields we need."""
    out = []
    for i in range(0, len(ids), batch_size):
        batch = ids[i:i + batch_size]
        resp = athena.batch_get_query_execution(QueryExecutionIds=batch)
        for qe in resp.get("QueryExecutions", []):
            status = qe.get("Status", {})
            stats = qe.get("Statistics", {})
            if not status.get("CompletionDateTime"):
                continue  # keep only completed
            out.append({
                "query_id": qe.get("QueryExecutionId"),
                "raw_query": qe.get("Query"),
                "state": status.get("State"),
                "started_at": status.get("SubmissionDateTime"),
                "finished_at": status.get("CompletionDateTime"),
                "engine_ms": stats.get("EngineExecutionTimeInMillis"),
                "bytes_scanned": stats.get("DataScannedInBytes"),
            })
    return out

# Usage: flatten IDs from the generator, then fetch details in batches
ids = []
for page in get_query_execution_ids(max_iterations=500):
    if not page:
        continue
    ids.extend(page)

executions = get_query_executions(ids)
```

<Notice type="warning" className="mt-6">
  Keep `max_iterations` conservative to control costs and latency. Add retries/backoff and time filtering (`started_at`) in production.
</Notice>

## 4. Sqlglot

<FancyLink linkText="sqlglot" url="https://github.com/tobymao/sqlglot" dark="true"/> is an open-source SQL parser and transpiler that converts SQL text into an **Abstract Syntax Tree (AST)**. It’s fast, stable, and supports a wide range of dialects — including Athena, BigQuery, Presto, and Snowflake — making it a strong foundation for static query analysis.

Under the hood, `sqlglot` leverages **Rust** for tokenization and parsing performance, allowing it to efficiently handle thousands of queries with minimal overhead. This makes it suitable for large-scale, production-grade workflows.

Unlike simple regex-based approaches, `sqlglot` fully understands SQL grammar. This allows you to:

* Parse complex queries with nested CTEs, subqueries, and window functions.
* Traverse and inspect every part of the query structure (tables, joins, predicates, etc.).
* Reuse the same parsing logic across different engines by switching dialects.

It’s used in popular tools like **dbt**, **DuckDB**, and **DataHub**, which speaks to its reliability and performance in production-grade pipelines.

In this project, `sqlglot` serves as the backbone for the **SqlAnalyzer** class — converting raw query text into a structured AST that can be iterated over to count joins, detect anti-patterns, and extract lineage information.

<Notice type="warning">
  While `sqlglot` can handle a large variety of dialects, some engine-specific extensions or macros (e.g., Athena comments or dbt Jinja syntax) may require pre-processing before parsing. Always sanitize queries before feeding them into the parser.
</Notice>

### 4.1 Minimal example: parsing and inspecting a query

```python
from sqlglot import parse_one
from sqlglot import exp

sql = """
SELECT u.id, COUNT(*)
FROM mycatalog.analytics.users u
JOIN mycatalog.analytics.orders o ON o.user_id = u.id
WHERE u.country LIKE 'ES%'
GROUP BY 1
ORDER BY 2 DESC
"""

# Parse once (Athena dialect in this project)
tree = parse_one(sql, read="athena")

# List fully-qualified tables
tables = []
for t in tree.find_all(exp.Table):
    if fq := f"{t.db}.{t.name}":
        tables.append(fq)

print(f"Tables used in the query: {tables}")
```

**What this shows**

* `parse_one(..., read="athena")` builds an AST you can traverse.
* `find_all(exp.Table)` retrieves table nodes; you can assemble `db.table` easily.

## 5. SqlAnalyzer (methods)

This section focuses **only on the methods** used to extract static insights from SQL text.
We’ll introduce the constructor and then group the public `get_*` methods by topic. Internal helpers (`_…`) are shown inline the first time they’re referenced (inside accordions).

### 5.0 Constructor

```python
class SqlAnalyzer:
    def __init__(self, query, engine="athena"):
        self.engine = engine
        self.tree = parse_one(query.strip(), read=self.engine)
```

### 5.1 Table lineage

Identify **read** sources and **write** targets.

```python
    def get_cte_names(self):
        names = set()
        for w in self.tree.find_all(exp.With):
            for cte in w.find_all(exp.CTE):
                name = None
                if cte.alias:
                    name = cte.alias
                elif getattr(cte, "this", None) and getattr(cte.this, "alias", None):
                    name = cte.this.alias
                if isinstance(name, str):
                    names.add(name.lower())
        return names

    def get_tables(self, include_cte=False, include_insert_target=False, include_create_target=False):
        cte_names = self.get_cte_names()
        out = set()
        for table in self.tree.find_all(exp.Table):
            if not include_insert_target and self._is_insert_target(table):
                continue
            if not include_create_target and self._is_create_target(table):
                continue
            if not include_cte and table.name and table.name.lower() in cte_names:
                continue
            fq = self._fq_from_tablelike(table)
            if fq:
                out.add(fq)
        return sorted(out)

    def get_insert_target(self):
        for ins in self.tree.find_all(exp.Insert):
            return self._fq_from_tablelike(ins.this)
        return None

    def get_create_target(self):
        for cr in self.tree.find_all(exp.Create):
            return self._fq_from_tablelike(cr.this)
        return None
```

<Accordion client:load title="Internal helpers used above">

```python
    def _is_create_target(self, table):
        node = table
        while node is not None:
            parent = node.parent
            if isinstance(parent, exp.Create):
                target = parent.this
                if node is target:
                    return True
                if isinstance(target, exp.Schema):
                    for x in target.find_all(exp.Table):
                        if node is x:
                            return True
            node = parent
        return False

    def _is_insert_target(self, table):
        node = table
        while node is not None:
            parent = node.parent
            if isinstance(parent, exp.Insert):
                target = parent.this
                if node is target:
                    return True
                if isinstance(target, exp.Schema):
                    for x in target.find_all(exp.Table):
                        if node is x:
                            return True
            node = parent
        return False

    def _fq_from_tablelike(self, node):
        if isinstance(node, exp.Table):
            fq = f"{node.db}.{node.name}".replace('"', "")
            return fq.lower() if fq else None
        if isinstance(node, exp.Schema):
            for t in node.find_all(exp.Table):
                return self._fq_from_tablelike(t)
        return None
```

</Accordion>

### 5.2 Join & union metrics

```python
    def get_joins(self, by_kind=True):
        total = 0
        kinds = {}
        for j in self.tree.find_all(exp.Join):
            total += 1
            kind = (j.args.get("kind") or "INNER").upper()
            kinds[kind] = kinds.get(kind, 0) + 1
        return total, (kinds if by_kind else {})

    def get_unions(self):
        total = 0
        union_all = 0
        for u in self.tree.find_all(exp.Union):
            total += 1
            is_distinct = u.args.get("distinct", True)
            if not is_distinct:
                union_all += 1
        return total, union_all, total - union_all
```

### 5.3 Structural complexity

```python
    def get_count_ctes_defined(self):
        return len(self.get_cte_names())

    def get_count_subqueries(self):
        return sum(1 for _ in self.tree.find_all(exp.Subquery))

    def get_count_exists_subqueries(self):
        return sum(1 for _ in self.tree.find_all(exp.Exists))

    def get_count_window_functions(self):
        return sum(1 for _ in self.tree.find_all(exp.Window))

    def get_count_max_ast_depth(self):
        maxd = 0
        stack = [(self.tree, 1)]
        seen = set()
        while stack:
            node, depth = stack.pop()
            nid = id(node)
            if nid in seen:
                continue
            seen.add(nid)
            if depth > maxd:
                maxd = depth
            for ch in node.iter_expressions():
                if ch is not None:
                    stack.append((ch, depth + 1))
        return maxd
```

### 5.4 Predicate & projection metrics

```python
    def get_columns_referenced_counts(self):
        total = 0
        distinct = set()
        for c in self.tree.find_all(exp.Column):
            total += 1
            t = getattr(c, "table", None)
            n = getattr(c, "name", None)
            if t and n:
                distinct.add(f"{t}.{n}".lower())
            elif n:
                distinct.add(str(n).lower())
            else:
                distinct.add(str(c).lower())
        return total, len(distinct)

    def get_count_projected_columns(self):
        cnt = 0
        for s in self.tree.find_all(exp.Select):
            exprs = s.expressions or []
            cnt += len(exprs)
        return cnt

    def get_count_or_chain_max(self):
        mx = 0
        for p in self._predicates():
            for node in p.find_all(exp.Or):
                mx = max(mx, self._flatten_or_terms(node))
        return mx

    def get_count_in_list_max(self):
        mx = 0
        for n in self.tree.find_all(exp.In):
            right = n.args.get("expressions") or n.args.get("query")
            if isinstance(right, exp.Tuple):
                mx = max(mx, len(right.expressions or []))
            elif isinstance(right, exp.Array):
                mx = max(mx, len(right.expressions or []))
        return mx

    def get_count_like_leading_wildcards(self):
        k = 0
        for n in self.tree.find_all(exp.Like):
            pat = n.args.get("expression") or n.args.get("right")
            if isinstance(pat, exp.Literal) and pat.is_string:
                s = (pat.this or "").strip().strip("'").strip('"')
                if s.startswith("%"):
                    k += 1
        return k

    def get_count_funcs_in_predicates(self):
        k = 0
        for p in self._predicates():
            for f in p.find_all(exp.Func):
                has_col = any(True for _ in f.find_all(exp.Column))
                if has_col:
                    k += 1
        return k
```

<Accordion client:load title="Internal helpers used above">

```python
    def _predicates(self):
        preds = []
        for where in self.tree.find_all(exp.Where):
            if where.this is not None:
                preds.append(where.this)
        for j in self.tree.find_all(exp.Join):
            on = j.args.get("on")
            if on is not None:
                preds.append(on)
        return preds

    def _flatten_or_terms(self, node):
        if not isinstance(node, exp.Or):
            return 1
        return self._flatten_or_terms(node.left) + self._flatten_or_terms(node.right)
```

</Accordion>

### 5.5 Best-practice flags

```python
    def get_has_select_star(self):
        return any(True for _ in self.tree.find_all(exp.Star))

    def get_has_select_distinct(self):
        for s in self.tree.find_all(exp.Select):
            if s.args.get("distinct"):
                return True
        return False

    def get_has_order_by_without_limit(self):
        for s in self.tree.find_all(exp.Select):
            has_order = s.args.get("order") is not None
            has_limit = s.args.get("limit") is not None
            if has_order and not has_limit:
                return True
        return False
```

### 5.6 Command classification

```python
COMMANDS = {
    "INSERT": exp.Insert,
    "CREATE": exp.Create,
    "ALTER": exp.Alter,
    "DROP": exp.Drop,
}

class SqlAnalyzer:
    # ...
    def get_commands(self):
        kinds = {}
        for cmd in self.tree.find_all(exp.Command):
            kind = str(getattr(cmd, "this", "")).upper()
            if kind:
                kinds[kind] = kinds.get(kind, 0) + 1
        for name, expression in COMMANDS.items():
            total = sum(1 for _ in self.tree.find_all(expression))
            if total:
                kinds[name] = kinds.get(name, 0) + total
        total = sum(kinds.values())
        return total, kinds
```

<Notice type="warning">
  These methods provide **static** insights. Correlate with runtime metrics (execution time, bytes scanned, rows output) from your engine to prioritize remediation.
</Notice>

## 6. Pipeline for Extracting Data

* Describe how queries are processed in batch (Prefect or ECS task).
* Mention parallelization strategy if applicable.
* Show how results are stored in a table (Bronze layer) and then transformed in dbt (Staging layer).
* Summarize how each query becomes an `AnalysisResult` record.

## 7. Next Steps

* Analyze trends: e.g., most common join types, frequency of SELECT \*, or AST depth distribution.
* Use the insights to establish internal SQL style and efficiency guidelines.
* Potential extensions: integrate cost metrics, visualize in Superset, or feed into a query quality dashboard.

## 8. (Optional) Learnings and Challenges

* Lessons learned from parsing thousands of queries.
* How to handle parsing errors, malformed queries, or recursive ASTs.
* Future improvements: caching, multiprocessing, or using sqlglot’s Rust tokenizer.
