---
slug: static-query-anlysis-sqlglot
title: Static Query Analysis with Sqlglot
meta_title: xx
description: xx
date: 2025-10-14
image: /images/blog/9999-glass-bridge.jpg
category: DE
tags: []
draft: false
---

## 1. Overview

This post explains how to extract and structure SQL query insights from Athena using **sqlglot** and a custom Python parser. The focus is on building a foundation for later performance analysis rather than interpreting the results yet.

The goal is to transform raw query history into a rich metadata dataset containing structural features, such as the number of joins, unions, CTEs, and flags for patterns like `SELECT *` or `ORDER BY` without `LIMIT`. These metrics help assess query complexity and prepare for identifying inefficient or costly query patterns across your organization.

This guide is especially relevant if you want to:

* Automatically parse large query logs (e.g., from Athena, BigQuery, or Snowflake).
* Build an internal SQL insights dataset for cost/performance optimization.
* Create a reproducible static analysis pipeline for SQL queries.

## 2. Problem

Data teams often accumulate thousands of historical SQL queries across BI tools, pipelines, and ad-hoc analysis. However, this data remains underutilized — stored as text without structure or insight.

Common challenges include:

* **Lack of visibility:** No systematic way to see which tables are used most, which queries are complex, or which teams produce heavy workloads.
* **Rising costs:** In systems like Athena, cost scales with data scanned. Inefficient queries (e.g., full-table scans, large joins, `SELECT *`) can silently drive up bills.
* **No feedback loop:** Engineers rarely get structured feedback about their query design.

The objective here is to **extract meaningful features from query text**. By parsing SQL with sqlglot, you can transform unstructured SQL into measurable metadata that supports:

* Identifying slow or expensive patterns.
* Designing best practices around query efficiency.
* Benchmarking how query design evolves over time.

The next sections detail the tools, parser, and pipeline that make this process scalable and maintainable.

<Notice type="warning">
  This approach focuses solely on **static analysis** — it inspects query text, not execution results. Runtime metrics like execution time, data scanned, or bytes processed should be retrieved directly from your query engine’s system tables or APIs to complement this analysis.
</Notice>

## 3. Extracting Query History (optional section)

* Explain how to retrieve Athena query logs (via `GetQueryExecution` or internal history tables).
* Mention what data you capture: query text, execution time, data scanned, etc.
* If scope feels too broad, skip this section and focus solely on static analysis.

## 4. Sqlglot

<FancyLink linkText="sqlglot" url="https://github.com/tobymao/sqlglot" dark="true"/> is an open-source SQL parser and transpiler that converts SQL text into an **Abstract Syntax Tree (AST)**. It’s fast, stable, and supports a wide range of dialects — including Athena, BigQuery, Presto, and Snowflake — making it a strong foundation for static query analysis.

Under the hood, `sqlglot` leverages **Rust** for tokenization and parsing performance, allowing it to efficiently handle thousands of queries with minimal overhead. This makes it suitable for large-scale, production-grade workflows.

Unlike simple regex-based approaches, `sqlglot` fully understands SQL grammar. This allows you to:

* Parse complex queries with nested CTEs, subqueries, and window functions.
* Traverse and inspect every part of the query structure (tables, joins, predicates, etc.).
* Reuse the same parsing logic across different engines by switching dialects.

It’s used in popular tools like **dbt**, **DuckDB**, and **DataHub**, which speaks to its reliability and performance in production-grade pipelines.

In this project, `sqlglot` serves as the backbone for the **SqlAnalyzer** class — converting raw query text into a structured AST that can be iterated over to count joins, detect anti-patterns, and extract lineage information.

<Notice type="warning">
  While `sqlglot` can handle a large variety of dialects, some engine-specific extensions or macros (e.g., Athena comments or dbt Jinja syntax) may require pre-processing before parsing. Always sanitize queries before feeding them into the parser.
</Notice>

### 4.1 Minimal example: parsing and inspecting a query

```python
from sqlglot import parse_one
from sqlglot import exp

sql = """
SELECT u.id, COUNT(*)
FROM mycatalog.analytics.users u
JOIN mycatalog.analytics.orders o ON o.user_id = u.id
WHERE u.country LIKE 'ES%'
GROUP BY 1
ORDER BY 2 DESC
"""

# Parse once (Athena dialect in this project)
tree = parse_one(sql, read="athena")

# List fully-qualified tables
tables = []
for t in tree.find_all(exp.Table):
    if fq := f"{t.db}.{t.name}":
        tables.append(fq)

print(f"Tables used in the query: {tables}")
```

**What this shows**

* `parse_one(..., read="athena")` builds an AST you can traverse.
* `find_all(exp.Table)` retrieves table nodes; you can assemble `db.table` easily.

## 5. SqlAnalyzer

* Present the class as the core of your analysis pipeline.
* Subsections for each group of methods:

  * **Table lineage:** extracting all referenced, inserted, and created tables.
  * **Join & union metrics:** count and classify joins/unions.
  * **Structural complexity:** count CTEs, subqueries, window functions, AST depth.
  * **Predicate & projection metrics:** number of columns, OR chains, LIKE wildcards, functions in predicates.
  * **Best-practice flags:** SELECT \*, ORDER BY without LIMIT, etc.
  * **Command classification:** detect INSERT, CREATE, UNLOAD, DROP.
* Explain how these metrics map to potential performance or maintainability issues.

## 6. Pipeline for Extracting Data

* Describe how queries are processed in batch (Prefect or ECS task).
* Mention parallelization strategy if applicable.
* Show how results are stored in a table (Bronze layer) and then transformed in dbt (Staging layer).
* Summarize how each query becomes an `AnalysisResult` record.

## 7. Next Steps

* Analyze trends: e.g., most common join types, frequency of SELECT \*, or AST depth distribution.
* Use the insights to establish internal SQL style and efficiency guidelines.
* Potential extensions: integrate cost metrics, visualize in Superset, or feed into a query quality dashboard.

## 8. (Optional) Learnings and Challenges

* Lessons learned from parsing thousands of queries.
* How to handle parsing errors, malformed queries, or recursive ASTs.
* Future improvements: caching, multiprocessing, or using sqlglot’s Rust tokenizer.
