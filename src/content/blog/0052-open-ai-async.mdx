---
slug: async-openai-calls-rate-limiter
title: Concurrent Async Calls to OpenAI with Rate Limiting
meta_title: How to Handle Concurrent OpenAI API Calls with Rate Limiting
description: Learn how to implement multiple concurrent async calls to the OpenAI API while managing quota limits effectively using a rate limiter in Python. This guide covers best practices for API management in data engineering.
date: 2024-08-22
image: /images/blog/0052-Async-calls-with-rate-limiter.jpg
category: DE
tags: [Python, API, Best Practices, OpenAI, DE]
draft: false
---

## 0. Motivation

xx

<FancyLink linkText="Process Calls with Open AI" url="https://villoro.com/blog/process-calls-open-ai/" dark="true"/>

## 1. Approach overview


## 2. Quota limits

### 2.1. Understanding OpenAI Rate Limits

OpenAI enforces rate limits to manage the number of API requests and tokens you can use within specific time frames.
These limits help ensure fair usage and stable performance for all users.
The key metrics to be aware of include:

1. **Requests Per Minute (RPM)**: The maximum number of API requests allowed per minute.
2. **Requests Per Day (RPD)**: The total number of API requests you can make in a day.
3. **Tokens Per Minute (TPM)**: The number of tokens (units of text) you can process per minute, including both input and output tokens.
4. **Tokens Per Day (TPD)**: The daily limit on token usage, covering all input and output tokens throughout the day.

The specific rate limits vary based on the subscription tier.
You can read more about it at <FancyLink linkText="Open AI | Rate Limits" url="https://platform.openai.com/docs/guides/rate-limits"/>.

### 2.2. Requests limits

The most important limit is the **Requests Per Minute (RPM)**.
That is specially important when calling `whisper-1` to transcribe audio since the RPM ranges from 3 to 500 depending on the tier.

### 2.1. Tokens limits

This other limit is important when calling `Chat GPT` (any version of it).
The number of tokens related to the number of words each request has.

When using a model with **Tokens Per Minute (TPM)** limits we will need to calculate the number of tokens needed before submitting a request.
I use a function that gives a good approximation for the tokens:

```python
import tiktoken

MODEL_GPT_4 = "gpt-4"
MODEL_TOKENIZER_DEFAULT = MODEL_GPT_4

def count_tokens(messages, model=MODEL_TOKENIZER_DEFAULT):
    """Estimate the number of tokens used by the messages."""

    # I belive the tokenization does not change between sub models
    if model.startswith(MODEL_GPT_4):
        model = MODEL_GPT_4

    encoding = tiktoken.encoding_for_model(model)

    tokens_per_message = 3  # A rough estimate for the start of each message
    tokens_per_name = 1  # If names are used, they add an extra token

    total_tokens = 0
    for message in messages:
        total_tokens += tokens_per_message
        for key, value in message.items():
            total_tokens += len(encoding.encode(value))
            if key == "name":
                total_tokens += tokens_per_name

    # Adding 3 tokens for the overall start and end of the message
    total_tokens += 3

    return total_tokens
```
<Notice type="warning">
  It does a good job at predicting tokens used but it's not perfect.
  Real usage will be slighly different.
</Notice>

## 3. Rate Limiter

We use a Rate Limiter to prevent exceeding API usage limits, ensuring compliance and avoiding throttling or service disruptions.
This approach maximizes the number of valid requests per minute.

### 3.1. How does it work?

* **Managing Requests and Tokens:** Before making an API call, the limiter checks if there are enough remaining requests or tokens. If limits are reached, it pauses execution until the limits reset.

* **Handling API Responses:** After each API call, the limiter updates its state using information from response headers, keeping internal counters accurate. More details on this are in the next section.

### 3.2. Overview of the RateLimiter

1. **Initialization**: The `RateLimiter` is initialized with optional limits for maximum requests (`max_requests`) and maximum tokens (`max_tokens`). These define the cap for how many requests or tokens can be used in a given period.

2. **Waiting for Availability**: The `wait_for_availability` method checks if there are enough remaining requests and tokens. If not, it pauses the execution for a calculated amount of time until limits are reset. This prevents the application from making too many API calls at once.

3. **Updating Limits**: The `update_limits` method resets the request and token counts if the reset time has passed, ensuring that the rate limits are restored after each time window.

4. **Dynamic Adjustment**: The `update_from_headers` method allows the rate limiter to adjust based on headers received from API responses, which may indicate the remaining requests, tokens, and the reset time. This keeps the rate limiter in sync with real-time API limits.

5. **Global Setup**: The `set_rate_limiter` function initializes a global `RATE_LIMITER` instance, making it easy to enforce rate limits across the application without passing the limiter object around.

<Notice type="info">
  There is some overlap between the `update_limits` and `update_from_headers` methods.
  While `update_limits` resets limits based on a fixed time window, `update_from_headers` allows dynamic adjustments based on real-time data from API responses.
  This overlap is intentional, as using `update_from_headers` ensures more accurate limit management and optimizes the number of valid requests per minute.
  By relying on actual API feedback, we can better handle fluctuating limits and reduce the risk of hitting rate caps unexpectedly.
</Notice>

### 3.2. Rate Limiter code

Here you have the full code for the `RateLimiter`:

<TerminalOutput color="stone">
  rate_limiter.py
</TerminalOutput>
```python
import asyncio
import re
import time

from loguru import logger


# Define a regular expression to capture time components
REGEX_TIME = re.compile(r"(?P<value>\d+)(?P<unit>[smhms]+)")

RATE_LIMITER = None
MAX_SLEEP_TIME = 2 * 60  # 2 Minutes


def set_rate_limiter(max_requests: int = None, max_tokens: int = None):
    global RATE_LIMITER
    RATE_LIMITER = RateLimiter(max_requests, max_tokens)


class RateLimiter:
    def __init__(self, max_requests: int = None, max_tokens: int = None):
        self.max_requests = max_requests
        self.max_tokens = max_tokens

        self.remaining_requests = max_requests
        self.remaining_tokens = max_tokens

        # Assume reset in 60 seconds initially
        self.reset_time_requests = time.monotonic() + 60
        self.reset_time_tokens = time.monotonic() + 60

        logger.info(f"Setting {self}")

    def __repr__(self):
        max_reqs = self.max_requests
        max_tokens = self.max_tokens
        rem_reqs = self.remaining_requests
        rem_tokens = self.remaining_tokens
        reset_t_reqs = round(self.reset_time_requests)
        reset_t_tokens = round(self.reset_time_tokens)

        if max_tokens is None:
            return f"RateLimiter({max_reqs=}, {rem_reqs=}, {reset_t_reqs=} [no_tokens])"

        return (
            f"RateLimiter({max_reqs=}, {max_tokens=} "
            f"{rem_reqs=}, {rem_tokens=}, {reset_t_reqs=}, {reset_t_tokens=})"
        )

    def _get_seconds_to_sleep(self):
        if self.max_tokens is None:
            # Only consider requests if tokens are not being used
            sleep_time = self.reset_time_requests - time.monotonic()
        else:
            sleep_time = min(
                self.reset_time_requests - time.monotonic(),
                self.reset_time_tokens - time.monotonic(),
            )

        # Do not wait really high times. Better to try anyway
        sleep_time = min(sleep_time, MAX_SLEEP_TIME)

        # Ensure sleep_time is at least 1 second
        return max(sleep_time, 1)

    async def wait_for_availability(self, required_tokens=None):
        if self.max_tokens is None:
            required_tokens = 0  # Ignore tokens if max_tokens is None

        while self.remaining_requests <= 0 or (
            self.max_tokens is not None and self.remaining_tokens < required_tokens
        ):
            self.update_limits()

            seconds_to_sleep = self._get_seconds_to_sleep()
            logger.debug(f"Sleeping {seconds_to_sleep=}")
            await asyncio.sleep(seconds_to_sleep)

        self.remaining_requests -= 1
        if self.max_tokens is not None:
            self.remaining_tokens -= required_tokens

    def update_limits(self):
        current_time = time.monotonic()

        # If we've passed the reset time, reset the limits
        if current_time >= self.reset_time_requests:
            self.remaining_requests = self.max_requests
            self.reset_time_requests = current_time + 60  # Reset the time window

        if self.max_tokens is not None and current_time >= self.reset_time_tokens:
            self.remaining_tokens = self.max_tokens
            self.reset_time_tokens = current_time + 60  # Reset the time window

        logger.debug(f"Updating limits:\n{self}")

    def _parse_reset_time(self, reset_time_str):
        """Convert a time reset string like '1s', '6m0s', or '60ms' into seconds."""

        total_seconds = 0
        for match in REGEX_TIME.finditer(reset_time_str):
            value = int(match.group("value"))
            unit = match.group("unit")

            if unit == "s":
                total_seconds += value
            elif unit == "m":
                total_seconds += value * 60
            elif unit == "h":
                total_seconds += value * 3600
            elif unit == "ms":
                total_seconds += value / 1000.0

        # Default to 60 seconds if the format is unexpected
        return total_seconds if total_seconds > 0 else 60

    def update_from_headers(self, headers):
        """Update the rate limits based on headers from the API response."""

        self.remaining_requests = int(
            headers.get("x-ratelimit-remaining-requests", self.remaining_requests)
        )

        if self.max_tokens is not None:
            self.remaining_tokens = int(
                headers.get("x-ratelimit-remaining-tokens", self.remaining_tokens)
            )

            reset_tokens_seconds = self._parse_reset_time(
                headers.get("x-ratelimit-reset-tokens", "60s")
            )
            self.reset_time_tokens = time.monotonic() + reset_tokens_seconds

        reset_requests_seconds = self._parse_reset_time(
            headers.get("x-ratelimit-reset-requests", "60s")
        )

        self.reset_time_requests = time.monotonic() + reset_requests_seconds
        logger.debug(f"Updating limits from headers:\n{self}")
```

## 4. Using the Rate Limiter with Async calls

As seen in <FancyLink linkText="Process Calls with Open AI" url="https://villoro.com/blog/process-calls-open-ai/" dark="true"/> we were using the <FancyLink linkText="OpenAI python" url="https://github.com/openai/openai-python" dark="true"/> package.
This simplifies the process but prevents us from getting relevant data from the headers (as explained in <FancyLink linkText="Rate limits in headers | Open AI" url="https://platform.openai.com/docs/guides/rate-limits/rate-limits-in-headers"/>).
That data is really useful for knowning:

* The remaining quota
* The time until the quota is reset

Since this will help us getting more accurate information which in turn will help us increase the number of requests we can do, we will do manual requests to the API.

### 4.1. Calling the API

We will use <FancyLink linkText="Aiohttp" url="https://github.com/aio-libs/aiohttp" dark="true"/> for making API requests.

The general idea is to have a function that waits for sufficient quota (managed by the `RateLimiter`) before calling the API.
If a `HTTP 429` error (rate limit exceeded) occurs, the function will recursively call itself to retry the request.
After each API call, it updates the `RateLimiter` with quota information from the response headers.

A semaphore is also used to limit the number of concurrent calls, ensuring that we don't overwhelm the system. More details on this will be covered later.

In the next section, you will see the full code for making calls to both Chat and Transcription endpoints.

### 4.2. Calling Transcription endpoint

For doing transcriptions we use `whisper-1` which doesn't have RPM limits.
So the code is simpler than the one for `Chat`.

Here is the code for it:

<TerminalOutput color="stone">
  transcription.py
</TerminalOutput>
```python
import asyncio
import io
import time
from datetime import datetime
from typing import Optional

import aiohttp
import pandas as pd
from mutagen.mp3 import HeaderNotFoundError
from mutagen.mp3 import MP3
from pydantic import BaseModel
from loguru import logger

import rate_limiter as rt
import utils as u

MIN_SECONDS = 5
DEFAULT_MODEL = "whisper-1"
ENDPOINT_TRANSCRIPTIONS = "https://api.openai.com/v1/audio/transcriptions"

MAX_REQUESTS_PER_MIN = 100

async def _call_openai_transcription(data):
    await rt.RATE_LIMITER.wait_for_availability()
    async with u.RATE_LIMITER_SEMAPHORE:  # Ensure no more than N tasks run concurrently
        async with aiohttp.ClientSession() as session:
            async with session.post(
                ENDPOINT_TRANSCRIPTIONS, headers=u.HEADERS, data=data
            ) as res:
                # Update the rate limiter with the response headers
                # We want this no matter if the request succeded or failed
                rt.RATE_LIMITER.update_from_headers(res.headers)

                try:
                    res.raise_for_status()

                except aiohttp.ClientResponseError as e:
                    if e.status == 429:
                        u.LOGGER.warning("Rate limit exceeded, retrying")
                        await rt.RATE_LIMITER.wait_for_availability()
                        return await _call_openai_transcription(data)
                    else:
                        raise e

                result = await res.json()
                return result["text"]

class TranscriptionOutput(BaseModel):
    """
    Export relevant metadata
    """

    filename: str
    duration_seconds: float
    transcription: Optional[str]
    transcription_time: Optional[float] = None
    transcribed_at: datetime = datetime.now()
    gpt_model: str


async def call_openai_transcription(filename, temperature=0.2, gpt_model=DEFAULT_MODEL):
    logger = get_run_logger()
    logger.info(f"Processing {filename=}")

    t0 = time.monotonic()
    file = io.BytesIO(s3.read_file(filename))
    duration_seconds = get_duration(file)

    transcription = None
    trans_time = None

    if duration_seconds > MIN_SECONDS:
        data = aiohttp.FormData()
        data.add_field(
            "file",
            file,
            filename=filename.split("/")[-1],
            content_type="audio/mpeg",
        )
        data.add_field("model", gpt_model)
        data.add_field("response_format", "json")
        data.add_field("temperature", str(temperature))

        transcription = await _call_openai_transcription(data)
        trans_time = time.monotonic() - t0
        logger.info(f"Successfully transcribed {filename=} in {trans_time:.2f} seconds")
    else:
        logger.info(
            f"Skipping transcription for {filename=}, "
            f"{duration_seconds=:.2f} is less than {MIN_SECONDS=}"
        )

    return TranscriptionOutput(
        filename=filename,
        duration_seconds=duration_seconds,
        transcription=transcription,
        transcription_time=trans_time,
        gpt_model=gpt_model,
    )
```

The `_call_openai_transcription` is the function that handles the Rate Limits.
We also have `call_openai_transcription` for taking care of both the input data to the API call. It also handles the output as a pydantic request (more info about that in <FancyLink linkText="Process Calls with Open AI" url="https://villoro.com/blog/process-calls-open-ai/" dark="true"/>). 

### 4.3. Calling Chat endpoint


### 4.4. Utils

<TerminalOutput color="stone">
  utils.py
</TerminalOutput>
```python
import asyncio

from loguru import logger

HEADERS = None
RATE_LIMITER_SEMAPHORE = asyncio.Semaphore(25)


def init_openai(secret, n_jobs=None, json=True):
    global HEADERS
    HEADERS = {"Authorization": f"Bearer {secret['api_key']}"}
    if json:
        HEADERS["Content-Type"] = "application/json"

    if n_jobs:
        global RATE_LIMITER_SEMAPHORE
        RATE_LIMITER_SEMAPHORE = asyncio.Semaphore(n_jobs)


def split_valid_and_invalid_records(records, pydantic_model):
    if invalid_results := [x for x in records if not isinstance(x, pydantic_model)]:
        logger.error(f"There are {len(invalid_results)} failed OpenAI calls")
    else:
        logger.info("All calls were successful")

    valid_results = [x for x in records if isinstance(x, pydantic_model)]
    return valid_results, invalid_results
```

## 5. Doing batches
