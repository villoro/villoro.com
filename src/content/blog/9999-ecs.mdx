---
slug: running-python-ecs-with-uv
title: Running Python Code in ECS with uv
meta_title: Building Fast and Reproducible ECS Python Deployments with uv
description: "Learn how to separate build and runtime stages using uv to create lightweight, versioned ECS environments. Package dependencies once, run anywhere."
date: 2025-10-24
image: /images/blog/9999-ship-python-containers.jpg
category: cloud_devops
tags: [AWS, ECS, Docker, uv, DE]
draft: false
---

## 0. Why Another ECS Setup?

Running Python workloads on ECS is simple in theory — but in practice, dependency builds slow everything down. Every task rebuilds the same environment, wasting time and compute.

The solution? **Package once, run anywhere.**

This post explains how to:

1. Build a **versioned virtual environment** once with `uv`.
2. Deploy a **minimal runtime container** that runs any version on ECS.
3. Automate setup with a simple shell script and S3.

<Notice type="info">
  By separating build and runtime, you get near-instant startup times, stable environments, and zero dependency drift between versions.
</Notice>

## 1. The Strategy

Run **small images** and **fetch the environment at runtime**. This plays to Fargate’s strengths (ephemeral tasks) and avoids paying the cold‑start penalty of large image pulls **every single run**.

### 1.0. What Fargate Optimizes For

* **No layer cache between tasks** → each task **pulls the full image** again.
* **Pull time scales with image size** → big images = slow starts.
* **Private subnets** without VPC endpoints can add NAT latency/cost.

<Notice type="info">
  Keep the image lean; move heavy Python deps out of the image. Treat S3 like a `layer store` for your virtualenv.
</Notice>

### 1.1. Two Approaches (and why one wins on Fargate)

| Approach                                     | What it means                                                | Pros                                                                                                        | Cons                                                                    |
| -------------------------------------------- | ------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------- |
| **A. Bake deps into the image**              | `pip install` during build; one fat image                    | Simple deploy; one immutable artifact                                                                       | Large image; slow pulls **every run**; rebuild image for any dep change |
| **B. Slim image + download venv at runtime** | Keep image minimal; fetch `venv_*.tar.gz` from S3 on startup | Tiny image → fast pulls; reuse one runtime image for many versions; faster iteration (swap venv, not image) | Slightly more bootstrap logic; needs S3 access & versioning             |

<Notice type="success">
  For short‑lived/batch tasks, **B** consistently starts faster: pull a ~80–100 MB image, then download a compressed venv from S3. Net startup is typically **tens of seconds**, not minutes.
</Notice>

### 1.2. The Pattern I Use (and this post documents)

**One image, many jobs/versions.** The Docker image is generic and reusable. The `version` you pass selects what to run.

**Package once, run anywhere**:

1. Build a **versioned** virtualenv tarball with `uv` (e.g., `venv_0.7.1.villoro.tar.gz`).
2. Publish **versioned artifacts** to S3 **as a set**:

   * `pyproject_<version>.toml`
   * `uv_<version>.lock`
   * `entrypoint_<version>.py`
   * `venv_<version>.tar.gz`
3. Ship a **minimal runtime** image that, on start:

   * downloads the selected version (`-v <version>`) from S3,
   * extracts to `./.venv/`,
   * activates it, and
   * runs the **versioned** `entrypoint.py` with your args.

<Notice type="success">
  Developers can iterate fast: compile a custom version, upload the four artifacts, and run it in prod with `-v <version>` with **no image rebuilds**. Roll forward/back by switching only the version.
</Notice>

### 1.3. Files & Layout (at a glance)

Files & Layout (at a glance)

```plaintext
deploy/
├── docker/
│   ├── Dockerfile.venv        # build the venv tarball with uv
│   └── Dockerfile.runtime     # tiny runtime image
└── scripts/
    ├── upload_all.py          # CI: push venv/locks/entrypoint to S3
    ├── download_all.py        # runtime: fetch artifacts by version
    └── setup_and_run.sh       # runtime: bootstrap → activate → run
ecs_northius/                  # Actual code, feel free to use whatever name you want
tests/
```

<Notice type="info" className="mt-6">
  Venv tarballs and lockfiles are **versioned** (e.g., `uv`, `0.7.1`). Run any version in prod with `-v <version>` which is great for hotfixes and A/B verification.
</Notice>

## 2. Building the venv (`Dockerfile.venv`)

We use **`uv`** for speed and determinism: it resolves from a lock file (`uv.lock`), creates a clean in-project venv, and installs **our code** into that venv so the tarball is fully self-contained for prod.

### 2.1 Why two base images (uv + python)?

* **`ghcr.io/astral-sh/uv` stage (distroless)** → provides only the `uv`/`uvx` binaries. We *copy* them in, avoiding `curl`/`apt` in the builder. This keeps the surface minimal and repeatable.
* **`python:3.12-slim-bookworm` builder** → gives us a small Debian-based Python with GNU `tar` and `gzip` to package the venv. We don’t pull compilers or OS build tools.

<Notice type="info">
  Copying the `uv` binaries (instead of installing `uv` with a package manager) **reduces image size**, **avoids network flakiness**, and keeps the build **hermetic**. The Python slim base is only used to create the venv and pack it.
</Notice>

### 2.2 Make the venv self-contained

Besides third‑party deps, we **install `ecs_northius` into the venv** (non‑editable). That means the resulting `venv_<version>.tar.gz` already contains *your* package and can run jobs in prod with no extra steps.

<Notice type="success">
  This is key: the venv tarball alone is enough to execute code in ECS; the runtime image only needs to download + activate it.
</Notice>

### 2.3 Dockerfile (optimized for cache & reproducibility)

```bash
# 0. Pull images
ARG BUILD_FOR=linux/amd64

# 0.1. Stage with just the uv binaries at /uv and /uvx (distroless)
FROM --platform=${BUILD_FOR} ghcr.io/astral-sh/uv:latest AS uvbin

# 0.2. Builder: slim Python + uv copied in
FROM --platform=${BUILD_FOR} python:3.12-slim-bookworm AS base

# 0.3. Set up config
WORKDIR /app/
ENV PYTHONIOENCODING=utf-8 \
    LANG=C.UTF-8 \
    UV_LINK_MODE=copy \
    UV_NO_PROGRESS=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1

# 1. Copy only the uv binaries (no curl, no apt)
COPY --from=uvbin /uv  /usr/local/bin/uv
COPY --from=uvbin /uvx /usr/local/bin/uvx

# 1.1. Test uv: fail fast if something changes upstream
RUN uv --version && uvx --version

# 2. Lock first for better cache hits
COPY pyproject.toml uv.lock ./

# 3. Create in-project venv and install deps (no dev if prod-only)
#    Use --copies to avoid symlink surprises when extracting the tarball later.
RUN uv venv --python 3.12 --copies && uv sync --frozen --no-dev

# 4. Add source and install it INTO the venv (non-editable)
COPY ecs_northius ./ecs_northius
RUN uv pip install --python .venv/bin/python --no-deps .

# 4.1. Test venv: sanity check
RUN .venv/bin/python -c "import ecs_northius; print('ecs_northius OK')"

# 5. Package the venv
ARG PACKAGE_VERSION
RUN mkdir -p /dist && tar -C .venv -czf /dist/venv_${PACKAGE_VERSION}.tar.gz .

# 6. Export only the venv tar
FROM scratch AS export
COPY --from=base /dist/venv_*.tar.gz /dist/
```

Build and export:

```bash
docker build -f deploy/docker/Dockerfile.venv --output . . --build-arg PACKAGE_VERSION=0.1.0 # whatever version
```

<Notice type="info" className="mt-6">
  The resulting `venv_0.1.0.tar.gz` is portable — upload it to S3 and reuse it across ECS tasks.
</Notice>

### 2.4 Extra optimizations & tricks

* **`.dockerignore` aggressively**: exclude `.git`, `tests/`, build artifacts, local caches. Smaller build context = faster.
* **Cache-friendly layering**: copy `pyproject.toml` + `uv.lock` **before** project sources to maximize `uv sync` cache hits.
* **Symlink safety**: `uv venv --copies` avoids symlink issues on extraction (pairs well with `tar.extractall(..., filter="fully_trusted")` at runtime).
* **Prune bytecode** *(optional)*: `find .venv -name "__pycache__" -type d -exec rm -rf {} +` to shave a few MB.
* **BuildKit caches** *(optional for repeated CI)*: with Docker BuildKit, you can use `--mount=type=cache,target=/root/.cache/uv` on the `uv sync` layer to speed rebuilds.

<Notice type="warning">
  Don’t install compilers or OS build tools in the builder unless you truly need to compile native deps. Prefer manylinux/prebuilt wheels to keep the venv portable and small.
</Notice>

## 3. The Runtime (`Dockerfile.runtime`)

Here the focus is speed — no build tools, no compilers, no uv.

```bash
FROM python:3.13-slim-bookworm AS runtime

WORKDIR /app
ENV PYTHONUNBUFFERED=1 PYTHONDONTWRITEBYTECODE=1 PIP_DISABLE_PIP_VERSION_CHECK=1

RUN python -m pip install --no-cache-dir boto3 click loguru toml

COPY scripts .

ENTRYPOINT ["/bin/bash", "./setup_and_run.sh"]
```

**plot_speed**

<Notice type="info">
  The runtime image is small and starts fast because dependency resolution happened during the venv build.
</Notice>

## 4. Runtime Bootstrap Script (`setup_and_run.sh`)

What ECS runs by default:

```bash
echo 1. Downloading venv and config
python download_all.py "$@"

echo 2. Activating venv
. ./.venv/bin/activate

echo 3. Running ECS task. Using args="$@"
python entrypoint.py "$@"
```

Key points:

* `$@` lets you pass a version: `-v uv`.
* No dependency resolution here; it only downloads and activates.
* Use LF endings to avoid `/bin/sh` errors:

  ```
  *.sh text eol=lf
  *.py text eol=lf
  ```

For dynamic entrypoint and parameter management, check out <FancyLink linkText="Effortless EMR: A Guide to Seamlessly Running PySpark Coder" url="https://villoro.com/blog/effortless-emr-guide-running-pyspark/" dark="true"/> — the concepts complement this ECS setup perfectly.

## 5. Download Script (`download_all.py`)

Fetches all artifacts from S3: venv, entrypoint, and config.

```python
import tarfile, boto3, click, utils as u
from loguru import logger

def download_s3(origin, dest, bucket=u.BUCKET):
    logger.info(f"Downloading from {origin=} to {dest=}")
    boto3.client("s3").download_file(bucket, origin, dest)

def venv_extract_tar(filename, local_venv=".venv"):
    logger.info(f"Extracting {filename=} to {local_venv=}")
    with tarfile.open(filename, "r:gz") as tar:
        tar.extractall(local_venv, filter="fully_trusted")

@click.command()
@click.option("--version", "-v", required=True, help="Version to download")
def download_all(version):
    logger.info(f"Downloading all files (version='{version}')")
    download_s3(f"{u.S3_UV}/pyproject_{version}.toml", "pyproject.toml")
    download_s3(f"{u.S3_UV}/uv_{version}.lock", "uv.lock")
    download_s3(f"{u.S3_ENTRY}/entrypoint_{version}.py", "entrypoint.py")
    download_s3(f"{u.S3_VENV}/venv_{version}.tar.gz", "venv.tar.gz")
    venv_extract_tar("venv.tar.gz")
    logger.success("All downloads done")

if __name__ == "__main__":
    download_all()
```

<Notice type="warning">
  Python 3.14+ tightens tar extraction security. Use `filter="fully_trusted"` to avoid `AbsoluteLinkError` from absolute symlinks.
</Notice>

### 1.4. How this connects to entrypoints & params

This post focuses on **environment packaging + fast starts** on ECS. For dynamic entrypoints (import-by-string), robust CLI handling with `click`, and typed params via `pydantic`, see <FancyLink linkText="Effortless EMR: A Guide to Seamlessly Running PySpark Coder" url="https://villoro.com/blog/effortless-emr-guide-running-pyspark/" dark="true"/>. The same entrypoint approach drops into this ECS runtime pattern without change.

<Notice type="info">
  Because **pyproject**, **uv.lock**, **venv**, and **entrypoint** are all versioned together, each run is **autonomous** and reproducible. You can safely test new jobs in production by pointing the runtime to a new version, then promote by updating the version used (or a `latest` alias).
</Notice>

## 6. How It Runs in ECS

Pass the version when starting the task:

```bash
docker run --rm ecs/runtime -v uv
```

At runtime the container:

1. Downloads `venv_<version>.tar.gz`, `entrypoint.py`, and lock files.
2. Extracts and activates the venv.
3. Runs the entrypoint with your arguments.

<Notice type="info">
  You can run different versions in parallel (e.g., `-v uv` for dev, `-v 0.7.1.villoro` for prod) without rebuilding images.
</Notice>

## 7. CI/CD Flow

* `Dockerfile.venv` builds and uploads the venv tarball to S3.
* `Dockerfile.runtime` builds and pushes the runtime image to ECR.
* GitHub Actions keeps these steps separate for fast, reproducible deploys.

<Notice type="info">
  The runtime image is reused across multiple versions, only downloading the desired venv from S3. This dramatically reduces cold starts.
</Notice>

## 8. Troubleshooting

| Problem                            | Cause                           | Fix                                 |
| ---------------------------------- | ------------------------------- | ----------------------------------- |
| `AbsoluteLinkError`                | Symlinked Python binary in venv | Build with `uv venv --copies`       |
| `cannot open ./.venv/bin/activate` | CRLF line endings               | Enforce LF via `.gitattributes`     |
| Slow startup                       | Re-downloading venv each run    | Consider caching or a shared volume |

## 9. Key Takeaways

This pattern gives you:

* Fast ECS startups (no runtime builds).
* Versioned reproducibility via S3 artifacts.
* Lightweight runtime images.
* Clean separation between build and execution.

With **uv**, **Docker**, and **S3**, your ECS deployments become as fast and predictable as your local runs.

<Notice type="info">
  The same venv tarballs work for Lambda, Batch, or local debugging for strong parity across environments.
</Notice>
