---
slug: async-qlik-api-python
title: "Async APIs with Python: Scaling Qlik Queries"
meta_title: "Async APIs with Python: Scaling Qlik Queries"
description: Learn how to scale Qlik Cloud automation with Python’s asyncio and aiohttp. This post builds an async client for concurrent reloads, pagination, retries, and error handling.
date: 2025-10-09
image: /images/blog/9999-qlik-multiple-magnifying-glasses.jpg
category: DE
tags: [Python, APIs, AsyncIO, Data Engineering]
draft: false
---

## 0. Intro

In the previous post we built a **synchronous client** for Qlik Cloud. It was simple, debuggable, and covered the basics: auth, pagination, reloads, and retries. But when you start to automate **multiple reloads** or poll **many apps at once**, sync becomes a bottleneck.

With Python’s `asyncio` and `aiohttp`, you can scale to **dozens or hundreds of API calls concurrently**, without threads or processes. This post shows how to rebuild the client with async foundations, keeping the same principles: copy/paste‑ready code, tight surface area, and clear seams for production use.

## 1. Async foundations

Why bother?

* **`requests`** is blocking. Each call waits for I/O.
* **`aiohttp`** is async. Calls yield control while waiting, letting others run.

Key concepts:

* `async def` defines a coroutine.
* `await` yields back to the event loop until the operation completes.
* `async with` manages async context (e.g. HTTP sessions).

<Notice type="info">
  Don’t mix `asyncio.run()` inside existing event loops (like Prefect or Jupyter). Only use it at the entrypoint of your script.
</Notice>

## 2. Rebuilding the client

We’ll mirror the synchronous client, but with async idioms.

```python
import aiohttp
import asyncio
import backoff
import uuid

class QlikAsyncClient:
    def __init__(self, timeout_s=180, page_size=100, max_calls=900, debug=False, concurrency=10):
        self.timeout_s = timeout_s
        self.page_size = page_size
        self.max_calls = max_calls
        self.debug = debug
        self._session = None
        self._sem = asyncio.Semaphore(concurrency)
        self.headers = {"Authorization": "Bearer ...", "Content-Type": "application/json"}

    async def __aenter__(self):
        await self._ensure_session()
        return self

    async def __aexit__(self, *args):
        if self._session and not self._session.closed:
            await self._session.close()

    async def _ensure_session(self):
        if not self._session or self._session.closed:
            timeout = aiohttp.ClientTimeout(total=self.timeout_s)
            self._session = aiohttp.ClientSession(headers=self.headers, timeout=timeout)
```

<Notice type="success">
  Use a semaphore (`asyncio.Semaphore`) to prevent accidental overload (e.g. sending 500 reloads at once).
</Notice>

## 3. Async GET helper

Same role as sync `_get`, but async:

```python
    @backoff.on_exception(backoff.expo, aiohttp.ClientError, max_tries=5)
    async def _get(self, endpoint=None, url=None, params=None):
        if not (endpoint or url):
            raise ValueError("Need either endpoint or url")
        if params and "limit" in params and params["limit"] > self.page_size:
            raise ValueError("Limit exceeds page_size")

        url = url or f"https://.../api/v1/{endpoint}"

        async with self._sem:
            async with self._session.get(url, params=params) as resp:
                if not self.debug:
                    resp.raise_for_status()
                if resp.status in (202, 204):
                    return {}
                return await resp.json()
```

## 4. Pagination, async style

```python
    async def query_all(self, endpoint, params=None):
        url = f"https://.../api/v1/{endpoint}"
        params = params or {"limit": self.page_size}

        calls = 0
        results = []

        while url:
            if calls >= self.max_calls:
                raise RuntimeError("Max API calls exceeded")

            data = await self._get(url=url, params=params)
            results.extend(data.get("data", []))
            url = data.get("links", {}).get("next", {}).get("href")
            calls += 1

        return results
```

<Notice type="info">
  Most Qlik endpoints paginate with `links.next.href`. Follow until `None`.
</Notice>

## 5. Triggering & polling reloads concurrently

Fire reloads, then poll in parallel:

```python
    async def reload_app(self, app_id):
        payload = {"appId": str(uuid.UUID(app_id))}
        async with self._sem:
            async with self._session.post("https://.../api/v1/reloads", json=payload) as resp:
                resp.raise_for_status()
                return await resp.json()

import time

MIN_POLLING_SECONDS = 60

    async def wait_for_reload(self, reload_id, poll_every_s=5, max_time_s=None):
        assert max_time_s is not None, "max_time_s is required"

        # Floors
        poll_every_s = max(1, int(poll_every_s))
        max_time_s = max(MIN_POLLING_SECONDS, int(max_time_s))

        start = time.monotonic()
        # ceil division; guarantee at least one iteration
        max_iters = max(int((max_time_s + poll_every_s - 1) // poll_every_s), 1)

        for i in range(max_iters):
            details = await self._get(endpoint=f"reloads/{reload_id}")
            status = details.get("status", "").upper()

            if status in {"SUCCEEDED", "COMPLETED", "SUCCESS"}:
                return details
            if status in {"FAILED", "ERROR", "ABORTED"}:
                raise RuntimeError(f"Reload {reload_id} failed: {details}")

            if time.monotonic() >= start + max_time_s:
                break

            await asyncio.sleep(poll_every_s)

        raise TimeoutError(f"Reload {reload_id} did not finish in {max_time_s} seconds")
```

Example: trigger many reloads at once

```python
async def main():
    async with QlikAsyncClient() as client:
        reloads = await asyncio.gather(
            *[client.reload_app(app_id) for app_id in ["id1", "id2", "id3"]]
        )
        statuses = await asyncio.gather(
            *[client.wait_for_reload(r["id"]) for r in reloads]
        )
        print(statuses)

asyncio.run(main())
```

## 6. Error handling & retries

* `backoff` works with async.
* Retry transient network errors, not business logic errors.
* Wrap only `_get`/`_post` so high-level methods stay clean.

<Notice type="warning">
  Don’t retry 400s or 403s—they won’t succeed no matter how many times you retry.
</Notice>

## 7. Optional: Notifications

Inject a callback to notify on error:

```python
async def slack_notifier(ctx):
    print("Send Slack message", ctx)

async def reload_with_notify(client, app_id):
    try:
        result = await client.reload_app(app_id)
        await client.wait_for_reload(result["id"])
    except Exception as e:
        await slack_notifier({"app_id": app_id, "error": str(e)})
        raise
```

## 8. Trade-offs and takeaways

* **Async = faster** when you need many concurrent calls.
* **Complexity = higher**, but isolated inside client.
* Start sync, move to async when:
  * You batch reloads.
  * You need responsive pipelines.
  * You hit throughput limits.

That’s it: you now have both a **sync** and **async** client for Qlik Cloud. Choose the right one for your workload, and extend from here (automation, orchestration, or error monitoring).
