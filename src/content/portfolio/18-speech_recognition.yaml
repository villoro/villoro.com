# --------------------------------------------------------------------------------------------------
# Basic metadata
# --------------------------------------------------------------------------------------------------
code: speech_recognition
title: Speech Recognition with Neural Networks
title_short: Speech Recognition
subtitle: Predict the transcription from raw audio.
image: speech_recognition.jpg
image_square: speech_recognition_square.png

main: False
highlight: True

tags:
  - AI
  - RNN
  - Natural Language Processing (NLP)
  - LSTM
  - Tensorflow

tags_filter:
  - AI
  - Natural Language Processing


# --------------------------------------------------------------------------------------------------
# Extra info. This will add a button with href to the url
# --------------------------------------------------------------------------------------------------
# link: 
#   text: Github
#   url:


# --------------------------------------------------------------------------------------------------
# Content
# --------------------------------------------------------------------------------------------------
brief_markdown: |
  The idea of this project was to create a full pipeline that would take raw sound and then predict the transcription.

  The first step I did was to work with different ways to extract features from audio.

  Then I tested different Neural Network (NN) architectures in order to crate a good model that could predict with the given features.

metrics:
  - 15 models trained
  - 2.2h average training time
  - Model with 1,946,957 parameters

motivation_markdown: |
  I did this project as a part of the [Artificial Intelligence Nanodegree](https://eu.udacity.com/course/ai-artificial-intelligence-nanodegree--nd898) (udacity moved the project to the [Natural Language Processing Nanodegree](https://eu.udacity.com/course/computer-vision-nanodegree--nd891)). The purpouse of this project was to learn how to process audio in order to extract features. One key aspect about sound recognition is the time.

body_markdown: |

  ## Pipeline

  The aim is to have a system that given raw audio is able to predict the transcription of the spoken language. The pipeline is summarized in this image:

  <div class="w3-center">
    <img src="/static/images/portfolio/speech_recognition_pipeline.png" alt="speech_recognition_pipeline" class="w3-image w3-padding-16" style="max-height: 300px;"/>
  </div>

  1. The first step is **pre-processing the raw audio**.
  2. The second is an **acoustic model** that takes audio features and returns a probability distribution of all possible transcriptions (letters and punctuation).
  3. The last step is to transform the probability and **create a transcription**.

  ### 1. Acoustic Features for Speech Recognition

  There are two ways to extract features from audio and both were tested to see which one gave the best results.

  The first one is to use the raw audio and transform it to a **spectogram** where one dimension represents time and the other one the frequency. This is done using [Fast Fourier Transform (FFT)](https://en.wikipedia.org/wiki/Fast_Fourier_transform).

  The secon one is to use [Mel-Frequency Cepstral Coefficients (MFCC)](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum). In general this consists of using only the frequencies that humans can recognize in order to reduce the dimensionality of the data.

  ### 2. Deep Neural Networks for Acoustic Modeling

  In this part I work with different Neural Network (NN) architectures to see which one performed better. There options ranged from pure Recurent Neural Networks (RNN) to combinations of Convolutional Neural Networks (CNN) + RNN. The models were:

  1. RNN
  2. RNN + TimeDistributed Dense
  3. CNN + RNN + TimeDistributed Dense
  4. RNN (2 layers) + TimeDistributed Dense
  5. Bidirectional RNN + TimeDistributed Dense

  And I ended up using a combination of all of the above:

      CNN + 2 Bidirectional RNN + 2 TimeDistributed Dense

  The full details of the NN can be seen in the next figure:

  <div class="w3-center">
    <img src="/static/images/portfolio/speech_recognition_network_structure.png" alt="speech_recognition_network_structure" class="w3-image w3-padding-16" style="max-height: 300px;"/>
  </div>

  > I used `dropout` and `batch normalization` to avoid overfitting and `Leaky ReLU` to avoid dead `ReLUs`.

  In order to get good results it is really important to note that this is a [Connectionist Temporal Classification (CTC)](https://en.wikipedia.org/wiki/Connectionist_temporal_classification) problem and the loss function needs to be a CTC loss.

  ### 3. Obtain the predictions

  I only use the most probable letter at each time. It is possible to increase the performance of the system a lot by adding a language model.

  > A language model would use the probabilities of each letter and do a matching with possible words. This will allow much better results.

  ## Results

  Below there is an example of the output and the real text.

  **True transcription:**

      mister quilter is the apostle of the middle classes and we are glad to welcome his gospel

  **Predicted transcription:**

      mis ter cilder is the aposol of the mitl clasos and were gllad to welkom his gosplel

  It is not very good but is able to understand some things. With a language model the results would be way better.
